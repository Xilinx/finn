{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINN - ModelWrapper and Analysis passes\n",
    "--------------------------------------\n",
    "<font size=\"3\"> This notebook is about the ModelWrapper class and analysis passes within FINN. \n",
    "\n",
    "Following showSrc function is used to print the source code of function calls in the Jupyter notebook:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def showSrc(what):\n",
    "    print(\"\".join(inspect.getsourcelines(what)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModelWrapper\n",
    "-------------------------\n",
    "* <font size=\"3\"> wrapper around ONNX ModelProto that exposes several utility\n",
    "    functions for graph manipulation and exploration </font>\n",
    "* <font size=\"3\"> ModelWrapper instance takes onnx model proto and `make_deepcopy` flag as input </font>\n",
    "* <font size=\"3\"> onnx model proto can either be a string with the path to a stored .onnx file on disk, or serialized bytes </font>\n",
    "* <font size=\"3\"> `make_deepcopy` is by default False but can be set to True if a (deep) copy should be created </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a ModelWrapper instance\n",
    "\n",
    "Here we use a premade ONNX file on disk to load up the ModelWrapper, but this could have been produced from e.g. a trained Brevitas PyTorch model. See [this notebook](brevitas-network-import.ipynb) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.core.modelwrapper import ModelWrapper\n",
    "model = ModelWrapper(\"LFCW1A1.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access the ONNX GraphProto through ModelWrapper\n",
    "\n",
    "ModelWrapper is a thin wrapper around the ONNX protobuf, and it offers a range of helper functions as well as direct access to the underlying protobuf. The `.model` member gives access to the full ONNX ModelProto, whereas `.graph` gives access to the GraphProto, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelProto IR version is 4\n",
      "GraphProto top-level outputs are [name: \"60\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_value: 1\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 10\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "]\n",
      "There are 29 nodes in the graph\n",
      "The first node is \n",
      "input: \"0\"\n",
      "output: \"21\"\n",
      "op_type: \"Shape\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# access the ONNX ModelProto\n",
    "modelproto = model.model\n",
    "print(\"ModelProto IR version is %d\" % modelproto.ir_version)\n",
    "\n",
    "# the graph\n",
    "graphproto = model.graph\n",
    "print(\"GraphProto top-level outputs are %s\" % str(graphproto.output))\n",
    "\n",
    "# the node list\n",
    "nodes = model.graph.node\n",
    "print(\"There are %d nodes in the graph\" % len(nodes))\n",
    "print(\"The first node is \\n%s\" % str(nodes[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for tensors\n",
    "<font size=\"3\"> Every input and output of every node in the onnx model is represented as tensor with several properties (i.e. name, shape, data type). ModelWrapper provides some utility functions to work with the tensors. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get all tensor names\n",
    "\n",
    "Produces a list of all tensor names (inputs, activations, weights, outputs...) in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', 'features.3.weight', 'features.3.bias', 'features.3.running_mean', 'features.3.running_var', 'features.7.weight', 'features.7.bias', 'features.7.running_mean', 'features.7.running_var', 'features.11.weight', 'features.11.bias', 'features.11.running_mean', 'features.11.running_var', '20', '23', '28', '30', '33', '34', '41', '42', '49', '50', '57', '58', '60']\n"
     ]
    }
   ],
   "source": [
    "# get all tensor names\n",
    "tensor_list = model.get_all_tensor_names()\n",
    "print(tensor_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Producer and consumer of a tensor\n",
    "\n",
    "A tensor can have a producer node and/or a consumer node in the onnx model. ModelWrapper provides two helper functions to access these nodes, they are shown in the following.\n",
    "\n",
    "It may be that a tensor does not have a producer or consumer node, for example if the tensor represents a constant that is already set. In that case `None` will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producer node of tensor 60:\n",
      "input: \"59\"\n",
      "input: \"58\"\n",
      "output: \"60\"\n",
      "op_type: \"Mul\"\n",
      "\n",
      "Consumer node of tensor 0:\n",
      "input: \"0\"\n",
      "output: \"21\"\n",
      "op_type: \"Shape\"\n",
      "\n",
      "Producer of tensor 0: None\n"
     ]
    }
   ],
   "source": [
    "# get random tensor and find producer and consumer (returns node)\n",
    "\n",
    "tensor_name = tensor_list[25]\n",
    "print(\"Producer node of tensor {}:\".format(tensor_name))\n",
    "print(model.find_producer(tensor_name))\n",
    "\n",
    "tensor_name = tensor_list[0]\n",
    "print(\"Consumer node of tensor {}:\".format(tensor_name))\n",
    "print(model.find_consumer(tensor_name))\n",
    "\n",
    "print(\"Producer of tensor 0: %s\" % str(model.find_producer(\"0\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tensor shape\n",
    "<font size=\"3\">Each tensor has a specific shape which can be accessed with the following ModelWrapper helper functions.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor 0 is [1, 1, 28, 28]\n"
     ]
    }
   ],
   "source": [
    "# get tensor_shape\n",
    "\n",
    "print(\"Shape of tensor 0 is %s\" % str(model.get_tensor_shape(\"0\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to set the tensor shape with a helper function. The syntax would be the following:\n",
    "    \n",
    "`onnx_model.set_tensor_shape(tensor_name, tensor_shape)`\n",
    "\n",
    "Optionally, the dtype (container datatype) of the tensor can also be specified as third argument. By default it is set to TensorProto.FLOAT. \n",
    "    \n",
    "**Important:** dtype should not be confused with FINN data type, which specifies the quantization annotation. See the remarks about FINN-ONNX in [this notebook](finn-basics.ipynb). It is safest to use floating point tensors as the container data type for best compatibility inside FINN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tensor FINN DataType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINN introduces its [own data types](https://github.com/Xilinx/finn/blob/dev/src/finn/core/datatype.py) because ONNX does not natively support precisions less than 8 bits. FINN is about quantized neural networks, so precision of i.e. 4 bits, 3 bits, 2 bits or 1 bit are of interest. To represent the data within FINN, float tensors are used with additional annotation to specify the quantized data type of a tensor. The following helper functions are about this quantization annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The FINN DataType of tensor 0 is DataType.FLOAT32\n",
      "The FINN DataType of tensor 32 is DataType.BIPOLAR\n"
     ]
    }
   ],
   "source": [
    "# get tensor data type (FINN data type)\n",
    "print(\"The FINN DataType of tensor 0 is \" + str(model.get_tensor_datatype(\"0\")))\n",
    "print(\"The FINN DataType of tensor 32 is \" + str(model.get_tensor_datatype(\"32\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the get_tensor_datatatype() function, the (FINN) datatype of a tensor can be set using the `set_tensor_datatype(tensor_name, datatype)` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tensor initializers\n",
    "Some tensors have *initializers*, like tensors that represent constants or i.e. the trained weight values. \n",
    "\n",
    "ModelWrapper contains two helper functions for this case, one to determine the current initializer and one to set the initializer of a tensor. If there is no initializer, `None` is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializer for tensor 33:\n",
      "[[ 1.  1.  1. ...  1.  1. -1.]\n",
      " [ 1.  1. -1. ...  1.  1. -1.]\n",
      " [-1.  1. -1. ... -1.  1. -1.]\n",
      " ...\n",
      " [-1.  1. -1. ... -1. -1.  1.]\n",
      " [ 1.  1. -1. ...  1.  1. -1.]\n",
      " [-1.  1.  1. ... -1. -1.  1.]]\n",
      "Initializer for tensor 0:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# get tensor initializer\n",
    "tensor_name = tensor_list[1]\n",
    "print(\"Initializer for tensor 33:\\n\" + str(model.get_initializer(\"33\")))\n",
    "print(\"Initializer for tensor 0:\\n\" + str(model.get_initializer(\"0\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">Like for the other tensor helper functions there is a `set_initializer(tensor_name, tensor_value)` function.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More helper functions\n",
    "ModelWrapper contains more useful functions, if you are interested please have a look at the [Python code](https://github.com/Xilinx/finn/blob/dev/src/finn/core/modelwrapper.py) directly. Additionally, in the folder notebooks/ a Jupyter notebook about transformation passes can be found.\n",
    "    \n",
    "In the following analysis passes are discussed in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis passes\n",
    "-------------------------\n",
    "* <font size=\"3\">traverses the graph structure and produces information about certain properties</font>\n",
    "* <font size=\"3\">input: ModelWrapper</font>\n",
    "* <font size=\"3\">returns dictionary of named properties that the analysis extracts</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - Quantity analysis of operation types\n",
    "<font size=\"3\">As an example, an analysis is designed that returns the number of nodes of the same operation types. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">First the model is shown to illustrate the analysis. For this netron is used. Netron is a visualizer for neural network, deep learning and machine learning models. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'LFCW1A1.onnx' at http://0.0.0.0:8081\n"
     ]
    }
   ],
   "source": [
    "import netron\n",
    "netron.start('LFCW1A1.onnx', port=8081, host=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"http://0.0.0.0:8081/\" style=\"position: relative; width: 100%;\" height=\"400\"></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"http://0.0.0.0:8081/\" style=\"position: relative; width: 100%;\" height=\"400\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">The onnx model has to be converted to a format that can be processed by FINN. This is done with ModelWrapper. As described in the short introduction, this is the format an analysis pass takes as input.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.core.modelwrapper import ModelWrapper\n",
    "model = ModelWrapper('LFCW1A1.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">The idea is to count all nodes that have the same operation type. The result should contain the operation types and the corresponding number of nodes that occur in the model. In the beginning an empty dictionary is created which is filled by the function and returned as result to the user at the end of the analysis.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_equal_nodes(model):\n",
    "    count_dict = {}\n",
    "    for node in model.graph.node:\n",
    "        if node.op_type in count_dict:\n",
    "            count_dict[node.op_type] +=1\n",
    "        else:\n",
    "            count_dict[node.op_type] = 1\n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">The function takes the model as input and iterates over the nodes. Then it is checked whether there is already an entry for the operation type in the dictionary. If this is not the case, an entry is created and set to `1`. If there is already an entry, it is incremented. If all nodes in the model have been iterated, the filled dictionary is returned.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">The analysis function of ModelWrapper is used to perform the analysis just designed. It is shown below and takes the function as input and performs it by passing the model to the function </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def analysis(self, analysis_fxn):\n",
      "        \"\"\"Run given anaylsis_fxn on this model and return resulting dict.\"\"\"\n",
      "        return analysis_fxn(self)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "showSrc(ModelWrapper.analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">The result can now simply be determined by calling the .analysis function.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Shape': 1, 'Gather': 1, 'Unsqueeze': 5, 'Concat': 1, 'Reshape': 1, 'Mul': 5, 'Sub': 1, 'Sign': 4, 'MatMul': 4, 'BatchNormalization': 3, 'Squeeze': 3}\n"
     ]
    }
   ],
   "source": [
    "print(model.analysis(count_equal_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
