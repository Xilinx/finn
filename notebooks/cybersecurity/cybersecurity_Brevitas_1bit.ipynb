{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Train a 1-bit MLP on the UNSW_NB15 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will show how to:\n",
    "* Create a Multi Layer Perceptron with binary weights and activations with Brevitas library.\n",
    "* Train the MLP on the quantized version of the UNSW_NB15 dataset.\n",
    "* Export the model to ONNX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The UNSW_NB15 Dataset Description\n",
    "* The [UNSW_NB15](https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/) is one of the latest dataset created by the Australian Centre for Cyber Security (ACCS). The IXIA PerfectStorm tool created more than 100GB of raw traffic. The total number of records is two million and 540,044 which are stored in four CSV files. However, only a subset is used, more specifically 257,673 records, split in two files: [\"UNSW_NB15_training-set.csv\"](https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/a%20part%20of%20training%20and%20testing%20set/UNSW_NB15_training-set.csv) and [\" UNSW_NB15_testing-set.csv\"](https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/a%20part%20of%20training%20and%20testing%20set/UNSW_NB15_testing-set.csv) which can be downloaded from the website.\n",
    "* A binary classification was applied, where the feature \"label\" would identify the record as an attack or as normal traffic.\n",
    "* Prior to quantization, all features that are strings were converted to numbers with integer encoding.\n",
    "* Quantization was done in python and mimics this [matlab script](https://github.com/TadejMurovic/BNN_Deployment/blob/master/cybersecurity_dataset_unswb15.m).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /workspace/.local/lib/python3.6/site-packages (1.1.4)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /opt/conda/lib/python3.6/site-packages (from pandas) (1.19.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: scikit-learn in /workspace/.local/lib/python3.6/site-packages (0.23.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /workspace/.local/lib/python3.6/site-packages (from scikit-learn) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /workspace/.local/lib/python3.6/site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.6/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.6/site-packages (from scikit-learn) (1.19.4)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (4.31.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user pandas\n",
    "!pip install --user scikit-learn\n",
    "!pip install --user tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "-------------\n",
    "1. [Define the quantized MLP model class](#create_model)\n",
    "2. [Define the Train and Test methods](#train_test)\n",
    "3. [Define Auxilary Method and Parameters ](#auxilary_&_parameters)\n",
    "4. [Define the Loss and Optimizer](#loss_optimizer)\n",
    "5. [Load the UNSW_NB15 dataset](#load_dataset)\n",
    "6. [Train, Test and see the Loss](#train_test_loss)\n",
    "7. [Change the model structure after training](#change_model)\n",
    "8. [Brevitas export](#brevitas_export)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define the Quantized MLP model class <a id='1create_model'></a>\n",
    "* The model is a quantized MLP model with 1-bit weights and 1-bit activations implemented with [Brevitas](https://github.com/Xilinx/brevitas). Brevitas is a PyTorch library for quantization-aware training.\n",
    "* The model has 4 FC layers, each layer has 593, 128, 64, 32 and 1 node.\n",
    "* Between each FC layer, Batchnorm is applied.\n",
    "* The activation function is a Quantized Identity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dependencies import value\n",
    "\n",
    "from brevitas.inject import BaseInjector as Injector\n",
    "from brevitas.core.bit_width import BitWidthImplType\n",
    "from brevitas.core.quant import QuantType\n",
    "from brevitas.core.restrict_val import RestrictValueType\n",
    "from brevitas.core.scaling import ScalingImplType\n",
    "\n",
    "class CommonQuant(Injector):\n",
    "    bit_width_impl_type = BitWidthImplType.CONST\n",
    "    scaling_impl_type = ScalingImplType.CONST\n",
    "    restrict_scaling_type = RestrictValueType.FP\n",
    "    scaling_per_output_channel = False\n",
    "    narrow_range = True\n",
    "    signed = True\n",
    "\n",
    "    @value\n",
    "    def quant_type(bit_width):\n",
    "        if bit_width is None:\n",
    "            return QuantType.FP\n",
    "        elif bit_width == 1:\n",
    "            return QuantType.BINARY\n",
    "        else:\n",
    "            return QuantType.INT\n",
    "\n",
    "class CommonWeightQuant(CommonQuant):\n",
    "    scaling_const = 1.0\n",
    "\n",
    "class CommonActQuant(CommonQuant):\n",
    "    min_val = -1.0\n",
    "    max_val = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.nn import QuantIdentity, QuantConv2d, QuantReLU, QuantLinear, QuantHardTanh\n",
    "import torch.nn as nn\n",
    "\n",
    "weight_bit_width = 1\n",
    "act_bit_width = 1\n",
    "\n",
    "class QuantMLP(nn.Module):\n",
    "    def __init__(self, input_size,hidden1, hidden2, hidden3, num_classes):\n",
    "        super(QuantMLP, self).__init__()\n",
    "        self.fc1   = QuantLinear(input_size, hidden1, bias=False, weight_bit_width=weight_bit_width, weight_quant=CommonWeightQuant)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(hidden1)\n",
    "        self.identity1 = QuantIdentity(act_quant=CommonActQuant, bit_width=act_bit_width)\n",
    "        \n",
    "        self.fc2   = QuantLinear(hidden1, hidden2, bias=False, weight_bit_width=weight_bit_width, weight_quant=CommonWeightQuant)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(hidden2)\n",
    "        self.identity2 = QuantIdentity(act_quant=CommonActQuant, bit_width=act_bit_width)\n",
    "        \n",
    "        self.fc3   = QuantLinear(hidden2, hidden3, bias=False, weight_bit_width=weight_bit_width, weight_quant=CommonWeightQuant)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(hidden3)\n",
    "        self.identity3 = QuantIdentity(act_quant=CommonActQuant, bit_width=act_bit_width)\n",
    "        \n",
    "        self.fc4   = QuantLinear(hidden3, num_classes, bias=False, weight_bit_width=weight_bit_width, weight_quant=CommonWeightQuant)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(num_classes)       \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        fc1 = self.fc1(x)\n",
    "        b1 = self.batchnorm1(fc1)\n",
    "        identity1 = self.identity1(b1)\n",
    "        \n",
    "        fc2 = self.fc2(identity1)\n",
    "        b2 = self.batchnorm2(fc2)\n",
    "        identity2 = self.identity2(b2)\n",
    "\n",
    "        fc3 = self.fc3(identity2)\n",
    "        b3 = self.batchnorm3(fc3)\n",
    "        identity3 = self.identity3(b3)\n",
    "        \n",
    "        fc4 = self.fc4(identity3)\n",
    "        b4 = self.batchnorm4(fc4)\n",
    "    \n",
    "        return b4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define Train and Test  methods  <a id='train_test'></a>\n",
    "* The train and test methods use the Dataloader, which loads the input of the model with a new batch in each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):        \n",
    "        inputs , target = data\n",
    "        optimizer.zero_grad()   \n",
    "                \n",
    "        #FORWARD PASS\n",
    "        output = model(inputs.float())\n",
    "        loss = criterion(output, target.unsqueeze(1))\n",
    "        \n",
    "        #BACKWARD AND OPTIMIZE        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # PREDICTIONS\n",
    "        pred = output.detach().numpy() > 0.5  \n",
    "        target = target.float()\n",
    "        y_true.extend(target.tolist()) \n",
    "        y_pred.extend(pred.reshape(-1).tolist())\n",
    "        \n",
    "        losses.append(loss.data.numpy()) \n",
    "        \n",
    "    model.eval()    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):    \n",
    "    model.eval() \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            \n",
    "            inputs ,target = data\n",
    "            output = torch.sigmoid(model(inputs.float()))  \n",
    "            \n",
    "            #PREDICTIONS\n",
    "            pred = (output.detach().numpy() > 0.5) *1\n",
    "            target = target.float()\n",
    "            y_true.extend(target.tolist()) \n",
    "            y_pred.extend(pred.reshape(-1).tolist())\n",
    "        \n",
    "    return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Define Auxilary Method and Parameters <a id='auxilary_&_parameters'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_loss_plot(losses):\n",
    "    x_axis = [i for i in range(len(losses))]\n",
    "    plt.plot(x_axis,losses)\n",
    "    plt.title('Loss of the model')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 593      \n",
    "hidden1 = 128      \n",
    "hidden2 = 64\n",
    "hidden3 = 32\n",
    "num_classes = 1    \n",
    "\n",
    "num_epochs = 1\n",
    "batch_size = 100 \n",
    "lr = 0.001 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Define the Loss and Optimizer <a id=\"loss_optimizer\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx \n",
    "import torch\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Load the UNSW_NB15 dataset <a id='load_dataset'></a>\n",
    "* This UNSW_NB15_quantized class implements a DataLoader class. This is useful because enables access to data in batches\n",
    "* This class contains a fully quantized dataset with values {0,1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the training and test set from the [official website](https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/a%20part%20of%20training%20and%20testing%20set/UNSW_NB15_training-set.csv\n",
    "!wget https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/a%20part%20of%20training%20and%20testing%20set/UNSW_NB15_testing-set.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([175341, 594])\n",
      "torch.Size([82332, 594])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from dataloader import UNSW_NB15\n",
    "from dataloader_quantized import UNSW_NB15_quantized\n",
    "\n",
    "file_path_train = \"UNSW_NB15_training-set.csv\"\n",
    "file_path_test = \"UNSW_NB15_testing-set.csv\"\n",
    "\n",
    "train_quantized_dataset = UNSW_NB15_quantized(file_path_train = file_path_train, \\\n",
    "                                              file_path_test = file_path_test, \\\n",
    "                                              train=True)\n",
    "train_quantized_loader = DataLoader(train_quantized_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_quantized_dataset = UNSW_NB15_quantized(file_path_train = file_path_train, \\\n",
    "                                              file_path_test = file_path_test, \\\n",
    "                                              train=False)\n",
    "test_quantized_loader = DataLoader(test_quantized_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train, Test the model and Verify the Loss <a id=\"train_test_loss\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:10<00:00, 10.07s/it]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = QuantMLP(input_size, hidden1, hidden2, hidden3, num_classes)\n",
    "\n",
    "running_loss = []\n",
    "model = QuantMLP(input_size, hidden1, hidden2, hidden3, num_classes)\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "        loss_epoch = train(model, train_quantized_loader, optimizer,criterion)\n",
    "        running_loss.append(loss_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48606860030121946"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model,test_quantized_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss_per_epoch = [np.mean(loss_per_epoch) for loss_per_epoch in running_loss]\n",
    "display_loss_plot(loss_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Change  the model after training <a id=\"change_model\"></a>\n",
    "* The model is trained on the UNSW_NB15 dataset, then it is altered: a Quantized Identity output layer will be added as well as the input will now accept {-1,1} as valid input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the state_dict\n",
    "path = \"quantized_mlp_unsw_nb15.pt\"\n",
    "torch.save(model.state_dict(), path)\n",
    "\n",
    "# load the state_dict and create new model with aditional QuantIdentity layer\n",
    "new_model_to_be = QuantMLP(input_size, hidden1, hidden2, hidden3, num_classes)\n",
    "new_model_to_be.load_state_dict(torch.load(path))  \n",
    "new_model_to_be.eval()\n",
    "\n",
    "class extended_model(nn.Module):\n",
    "    def __init__(self, my_pretrained_model):\n",
    "        super(extended_model, self).__init__()\n",
    "        self.pretrained = my_pretrained_model\n",
    "        self.identity = QuantIdentity(act_quant=CommonActQuant, bit_width=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = (x + torch.tensor([1.0])) / 2.0  # shift from {-1,1} {0,1} \n",
    "        out_original = self.pretrained(x)\n",
    "        out_final = self.identity(out_original)   # output as {-1,1}     \n",
    "        return out_final\n",
    "\n",
    "new_model = extended_model(my_pretrained_model=new_model_to_be)\n",
    "new_model.eval()\n",
    "new_model_output = new_model.forward(test_quantized_dataset.data[:,:-1] * 2.0 - torch.tensor([1.0])) # feed data as {-1,1}\n",
    "new_model_output = new_model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Brevitas export <a id=\"brevitas_export\" ></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINN expects an ONNX model as input. This can be a model trained with [Brevitas](https://github.com/Xilinx/brevitas). First a few things have to be imported. Then the model can be loaded with the pretrained weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to brevitas_w1_a1NSW_NB15_model.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:17: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n"
     ]
    }
   ],
   "source": [
    "import brevitas.onnx as bo\n",
    "\n",
    "export_onnx_path = \"brevitas_w%d_a%-uNSW_NB15_model.onnx\" % (weight_bit_width, act_bit_width)\n",
    "input_shape = (1, 593)\n",
    "bo.export_finn_onnx(new_model, input_shape, export_onnx_path)\n",
    "\n",
    "new_model.eval\n",
    "print(\"Model saved to %s\" % export_onnx_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
