{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /workspace/.local/lib/python3.6/site-packages (1.1.4)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /opt/conda/lib/python3.6/site-packages (from pandas) (1.19.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: scikit-learn in /workspace/.local/lib/python3.6/site-packages (0.23.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /workspace/.local/lib/python3.6/site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.6/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.6/site-packages (from scikit-learn) (1.19.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /workspace/.local/lib/python3.6/site-packages (from scikit-learn) (0.17.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (4.31.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user pandas\n",
    "!pip install --user scikit-learn\n",
    "!pip install --user tqdm\n",
    "\n",
    "#do not change this order\n",
    "import onnx \n",
    "import torch \n",
    "\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "#\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "from brevitas.nn import QuantIdentity, QuantConv2d, QuantReLU, QuantLinear, QuantHardTanh\n",
    "from brevitas.core.quant import QuantType\n",
    "\n",
    "from dataloader import UNSW_NB15\n",
    "from dataloader_quantized import UNSW_NB15_quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspired by [this github file](https://github.com/alik604/cyber-security/blob/master/Intrusion-Detection/UNSW_NB15%20-%20Torch%20MLP%20and%20autoEncoder.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get UNSW_NB15 train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/a%20part%20of%20training%20and%20testing%20set/UNSW_NB15_training-set.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/a%20part%20of%20training%20and%20testing%20set/UNSW_NB15_testing-set.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dependencies import value\n",
    "\n",
    "from brevitas.inject import BaseInjector as Injector\n",
    "from brevitas.core.bit_width import BitWidthImplType\n",
    "from brevitas.core.quant import QuantType\n",
    "from brevitas.core.restrict_val import RestrictValueType\n",
    "from brevitas.core.scaling import ScalingImplType\n",
    "\n",
    "class CommonQuant(Injector):\n",
    "    bit_width_impl_type = BitWidthImplType.CONST\n",
    "    scaling_impl_type = ScalingImplType.CONST\n",
    "    restrict_scaling_type = RestrictValueType.FP\n",
    "    scaling_per_output_channel = False\n",
    "    narrow_range = True\n",
    "    signed = True\n",
    "\n",
    "    @value\n",
    "    def quant_type(bit_width):\n",
    "        if bit_width is None:\n",
    "            return QuantType.FP\n",
    "        elif bit_width == 1:\n",
    "            return QuantType.BINARY\n",
    "        else:\n",
    "            return QuantType.INT\n",
    "\n",
    "\n",
    "class CommonWeightQuant(CommonQuant):\n",
    "    scaling_const = 1.0\n",
    "\n",
    "\n",
    "class CommonActQuant(CommonQuant):\n",
    "    min_val = -1.0\n",
    "    max_val = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Neural Network class\n",
    "weight_bit_width = 1\n",
    "act_bit_width = 1\n",
    "\n",
    "class QuantLeNet(nn.Module):\n",
    "    def __init__(self, input_size,hidden1, hidden2, hidden3, num_classes):\n",
    "        super(QuantLeNet, self).__init__()\n",
    "        self.fc1   = QuantLinear(input_size, hidden1, bias=False, weight_bit_width=weight_bit_width, weight_quant=CommonWeightQuant)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(hidden1)\n",
    "        self.relu1 = QuantIdentity(act_quant=CommonActQuant, bit_width=act_bit_width)\n",
    "        \n",
    "        self.fc2   = QuantLinear(hidden1, hidden2, bias=False, weight_bit_width=weight_bit_width, weight_quant=CommonWeightQuant)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(hidden2)\n",
    "        self.relu2 = QuantIdentity(act_quant=CommonActQuant, bit_width=act_bit_width)\n",
    "        \n",
    "        self.fc3   = QuantLinear(hidden2, hidden3, bias=False, weight_bit_width=weight_bit_width, weight_quant=CommonWeightQuant)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(hidden3)\n",
    "        self.relu3 = QuantIdentity(act_quant=CommonActQuant, bit_width=act_bit_width)\n",
    "        \n",
    "        self.fc4   = QuantLinear(hidden3, num_classes, bias=False, weight_bit_width=weight_bit_width, weight_quant=CommonWeightQuant)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(num_classes)     \n",
    "\n",
    "    def forward(self, x):\n",
    "        fc1 = self.fc1(x)\n",
    "        b1 = self.batchnorm1(fc1)\n",
    "        relu1 = self.relu1(b1)\n",
    "        \n",
    "        fc2 = self.fc2(relu1)\n",
    "        b2 = self.batchnorm2(fc2)\n",
    "        relu2 = self.relu2(b2)\n",
    "\n",
    "        fc3 = self.fc3(relu2)\n",
    "        b3 = self.batchnorm3(fc3)\n",
    "        relu3 = self.relu3(b3)\n",
    "        \n",
    "        fc4 = self.fc4(relu3)\n",
    "        b4 = self.batchnorm4(fc4)\n",
    "        #import pdb; pdb.set_trace()\n",
    "        return b4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Train,   Test   and    Display_Loss_Plot    methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):        \n",
    "        # get the inputs; data is a list of [inputs, target ( or labels)]\n",
    "        inputs , target = data\n",
    "        optimizer.zero_grad()   \n",
    "                \n",
    "        #FORWARD PASS\n",
    "        output = model(inputs.float())\n",
    "        loss = criterion(output, target.unsqueeze(1))\n",
    "        \n",
    "        #BACKWARD AND OPTIMIZE        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        # PREDICTIONS\n",
    "        #pred = np.round(output.detach().numpy())\n",
    "        pred = output.detach().numpy() > 0.5  \n",
    "        target = target.float()\n",
    "        y_true.extend(target.tolist()) \n",
    "        y_pred.extend(pred.reshape(-1).tolist())\n",
    "        \n",
    "        losses.append(loss.data.numpy()) \n",
    "    #print(\"Accuracy on training set is\" , accuracy_score(y_true,y_pred))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING THE MODEL\n",
    "def test(model, device, test_loader):    \n",
    "    model.eval()   #model in eval mode skips Dropout etc\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "   \n",
    "    with torch.no_grad(): # set the requires_grad flag to false as we are in the test mode\n",
    "        for data in test_loader:\n",
    "            \n",
    "            #LOAD THE DATA IN A BATCH\n",
    "            inputs ,target = data\n",
    "            \n",
    "            # the model on the data\n",
    "            output = torch.sigmoid(model(inputs.float()))  \n",
    "            \n",
    "            #PREDICTIONS\n",
    "            pred = np.round(output)\n",
    "            #pred = output.detach().numpy() > 0.5 \n",
    "            #pred = pred * 1\n",
    "            target = target.float()\n",
    "            y_true.extend(target.tolist()) \n",
    "            y_pred.extend(pred.reshape(-1).tolist())\n",
    "        \n",
    "    return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_loss_plot(losses):\n",
    "    x_axis = [i for i in range(len(losses))]\n",
    "    plt.plot(x_axis,losses)\n",
    "    plt.title('Loss of the model')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('Cross entropy loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define some parameters first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "input_size = 593      #\n",
    "hidden1 = 128      # 1st layer number of neurons\n",
    "hidden2 = 64\n",
    "hidden3 = 32\n",
    "num_classes = 1    # binary classification\n",
    "\n",
    "num_epochs = 3\n",
    "batch_size = 100 \n",
    "lr = 0.001        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Neural Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QuantLeNet(input_size, hidden1, hidden2, hidden3, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define loss and optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize UNSW_NB15 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([82332, 594])\n"
     ]
    }
   ],
   "source": [
    "#Get the Quantized versions \n",
    "#train_quantized_dataset = UNSW_NB15_quantized(file_path_train='data/UNSW_NB15_training-set.csv', \\\n",
    "#                                              file_path_test = \"data/UNSW_NB15_testing-set.csv\", \\\n",
    "#                                              train=True)\n",
    "#train_quantized_loader = DataLoader(train_quantized_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_quantized_dataset = UNSW_NB15_quantized(file_path_train='data/UNSW_NB15_training-set.csv', \\\n",
    "                                              file_path_test = \"data/UNSW_NB15_testing-set.csv\", \\\n",
    "                                              train=False)\n",
    "test_quantized_loader = DataLoader(test_quantized_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Train, Test the model and see the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "running_loss = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "        loss_epoch = train(model, device, train_quantized_loader, optimizer,criterion)\n",
    "        running_loss.append(loss_epoch)\n",
    "#Save the model!!\n",
    "torch.save(model.state_dict(), \"MLP_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load(\"MLP_model\"))\n",
    "test(model,device,test_quantized_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_per_epoch = [np.mean(loss_per_epoch) for loss_per_epoch in running_loss]\n",
    "display_loss_plot(loss_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**********************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create files to verify the model after Brevitas export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.6249],\n",
       "        [ 0.0040],\n",
       "        [ 0.7298],\n",
       "        ...,\n",
       "        [-0.7218],\n",
       "        [ 0.7298],\n",
       "        [-2.1733]], grad_fn=<NativeBatchNormBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_output = model.forward(test_quantized_dataset.data[:,:-1])\n",
    "np.savetxt(\"brevitas_1_bit_model_no_sigmoid.csv\", raw_output.detach().numpy(), delimiter=\",\")\n",
    "raw_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Brevitas model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to brevitas_w1_a1NSW_NB15_model.onnx\n"
     ]
    }
   ],
   "source": [
    "#do not change this order\n",
    "import onnx \n",
    "import torch \n",
    "import brevitas.onnx as bo\n",
    "\n",
    "export_onnx_path = \"brevitas_w%d_a%-uNSW_NB15_model.onnx\" % (weight_bit_width, act_bit_width)\n",
    "#export_onnx_path = \"brevitas1BitUnswnb15.onnx\"\n",
    "input_shape = (2287, 593)\n",
    "bo.export_finn_onnx(model, input_shape, export_onnx_path)\n",
    "print(\"Model saved to %s\" % export_onnx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
