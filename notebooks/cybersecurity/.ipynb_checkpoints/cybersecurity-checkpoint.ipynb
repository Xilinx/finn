{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#needed to create the Neural Network\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#general\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get UNSW_NB15 train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/a%20part%20of%20training%20and%20testing%20set/UNSW_NB15_training-set.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/a%20part%20of%20training%20and%20testing%20set/UNSW_NB15_testing-set.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define UNSW_NB15 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNSW_NB15(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_path, sequence_length=25, transform=None):\n",
    "        #TODO have a sequence_overlap=True flag? Does overlap matter?\n",
    "        self.transform = transform\n",
    "        self.sequence_length = sequence_length\n",
    "        self.columns = ['id', 'dur', 'proto', 'service', 'state', 'spkts', 'dpkts', 'sbytes',\n",
    "       'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss',\n",
    "       'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin',\n",
    "       'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth',\n",
    "       'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm',\n",
    "       'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm',\n",
    "       'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm',\n",
    "       'ct_srv_dst', 'is_sm_ips_ports', 'attack_cat', 'label']\n",
    "        self.dtypes = dtypes = {\"id\":\"int32\",\n",
    "                                \"scrip\": \"string\",\n",
    "                                #\"sport\": \"int32\",\n",
    "                                \"dstip\": \"string\",\n",
    "                                #\"dsport\": \"int32\",\n",
    "                                \"proto\": \"string\",\n",
    "                                \"state\": \"string\",\n",
    "                                \"dur\": \"float64\",\n",
    "                                \"sbytes\": \"int32\",\n",
    "                                \"dbytes\": \"int32\",\n",
    "                                \"sttl\": \"int32\",\n",
    "                                \"dttl\": \"int32\",\n",
    "                                \"sloss\": \"int32\",\n",
    "                                \"dloss\": \"int32\",\n",
    "                                \"service\": \"string\",\n",
    "                                \"sload\": \"float64\",\n",
    "                                \"dload\": \"float64\",\n",
    "                                \"spkts\": \"int32\",\n",
    "                                \"dpkts\": \"int32\",\n",
    "                                \"swin\": \"int32\",\n",
    "                                \"dwin\": \"int32\",\n",
    "                                \"stcpb\": \"int32\",\n",
    "                                \"dtcpb\": \"int32\", \n",
    "                                #\"smeansz\": \"int32\",\n",
    "                                #\"dmeansz\": \"int32\",\n",
    "                                \"trans_depth\": \"int32\",\n",
    "                                #\"res_bdy_len\": \"int32\",\n",
    "                                \"sjit\": \"float64\",\n",
    "                                \"djit\": \"float64\",\n",
    "                                #\"stime\": \"int64\",\n",
    "                                #\"ltime\": \"int64\",\n",
    "                                #\"sintpkt\": \"float64\",\n",
    "                                #\"dintpkt\": \"float64\",\n",
    "                                \"tcprtt\": \"float64\",\n",
    "                                \"synack\": \"float64\",\n",
    "                                \"ackdat\": \"float64\",\n",
    "\n",
    "                                #commenting these because they have mixed values and we aren't going to generate them anyway\n",
    "                                #\"is_sm_ips_ports\": \"int32\",\n",
    "                                #\"ct_state_ttl\": \"int32\",\n",
    "                                #\"ct_flw_httpd_mthd\": \"int32\",\n",
    "                                #\"is_ftp_login\": \"int32\",\n",
    "                                #\"is_ftp_cmd\": \"int32\",\n",
    "                                #\"ct_ftp_cmd\": \"int32\",\n",
    "                                #\"ct_srv_src\": \"int32\",\n",
    "                                ##\"ct_dst_ltm\": \"int32\", \n",
    "                                #\"ct_src_ltm\": \"int32\",\n",
    "                                #\"ct_src_dport_ltm\": \"int32\",\n",
    "                                #\"ct_dst_sport_ltm\": \"int32\",\n",
    "                                #\"ct_dst_src_ltm\": \"int32\",\n",
    "                                \"attack_cat\": \"string\",\n",
    "                                \"label\": \"int32\"}\n",
    "        self.categorical_column_values = {\"proto\":None, \"state\":None, \"service\":None, \"attack_cat\":None}\n",
    "\n",
    "        self.dataframe = pd.read_csv(file_path, encoding=\"latin-1\", names=self.columns,header=0, dtype=self.dtypes)\n",
    "        #self.dataframe.sort_values(by=['stime']) #sort chronologically upon loading\n",
    "        \n",
    "        #load all the unique values of categorical features at the start\n",
    "        #and make these accessible via a fast function call.\n",
    "        for key in self.categorical_column_values:\n",
    "            self.categorical_column_values[key] = self.dataframe[key].unique()\n",
    "\n",
    "        #cache all the maximum values in numeric columns since we'll be using these for feature extraction\n",
    "        self.maximums = {}\n",
    "        for key in self.dtypes:\n",
    "            if \"int\" in self.dtypes[key] or \"float\" in self.dtypes[key]:\n",
    "                self.maximums[key] = max(self.dataframe[key])\n",
    "        \n",
    "        #------------------------------------------------\n",
    "        self.dataframe = self.dataframe.drop(['id'],1)\n",
    "               \n",
    "       \n",
    "        ##------Encoding string columns with value between 0 and n_classes-1----\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        self.dataframe['attack_cat'] = le.fit_transform(self.dataframe['attack_cat'])\n",
    "        self.dataframe['proto'] = le.fit_transform(self.dataframe['proto'])\n",
    "        self.dataframe['service'] = le.fit_transform(self.dataframe['service'])\n",
    "        self.dataframe['state'] = le.fit_transform(self.dataframe['state'])\n",
    "        \n",
    "        # ----------Normalising all numerical features--------------\n",
    "        #cols_to_normalise = list(self.dataframe.columns.values)[:39]\n",
    "        #self.dataframe[cols_to_normalise] = self.dataframe[cols_to_normalise].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "        #self.dataframe[cols_to_normalise] = self.dataframe[cols_to_normalise].apply(lambda x: (x - x.min()) / (x.max() - x.min()))               \n",
    "        \n",
    "        #-----------Create pytorch tensor----------------\n",
    "        self.tensor = torch.Tensor(self.dataframe.values)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_tensor(self):\n",
    "        return self.tensor\n",
    "    \n",
    "    def get_dataframe(self):\n",
    "        return self.dataframe\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe.index) - self.sequence_length\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #TODO need error checking for out of bounds?\n",
    "        #TODO return x,y where y is the category of the example\n",
    "        #since none corresponds to \"normal\" data\n",
    "        \n",
    "        list_of_dicts = []\n",
    "        for i in range(index,index+self.sequence_length):\n",
    "            list_of_dicts.append(self.dataframe.loc[i, :].to_dict())\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            return self.transform(self, list_of_dicts)\n",
    "        \n",
    "        return list_of_dicts\n",
    "    \n",
    "    #get a list of all the unique labels in the dataset\n",
    "    def get_labels(self):\n",
    "        return self.dataframe['label'].unique().tolist()\n",
    "    \n",
    "    #get a list of all the unique attack categories in the dataset\n",
    "    def get_attack_categories(self):\n",
    "        return self.dataframe['attack_cat'].unique().tolist()\n",
    "    \n",
    "    def get_list_of_categories(self, column_name):\n",
    "        pass #TODO\n",
    "\n",
    "    #limit the dataset to only examples in the specified category\n",
    "    def use_only_category(self, category_name):\n",
    "        if category_name not in self.get_attack_categories():\n",
    "            return False\n",
    "        \n",
    "        new_dataframe = self.dataframe[self.dataframe['attack_cat'] == category_name]\n",
    "        new_dataframe = new_dataframe.reset_index()\n",
    "        self.dataframe = new_dataframe\n",
    "        return True\n",
    "    \n",
    "    #limit the dataset to only examples with the specified label\n",
    "    def use_only_label(self, label):\n",
    "        if label not in self.get_labels():\n",
    "            return False\n",
    "        \n",
    "        new_dataframe = self.dataframe[self.dataframe['label'] == label]\n",
    "        new_dataframe = new_dataframe.reset_index()\n",
    "        self.dataframe = new_dataframe\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Neural Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define NN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, hidden_size_2, num_classes):\n",
    "        super(Net,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        # linear layer (input_size -> hidden_size)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        # linear layer (hidden_size -> hidden_2)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size_2)\n",
    "        # linear layer (hidden_size_2 -> num_classes)\n",
    "        self.fc3 = nn.Linear(hidden_size_2, num_classes)\n",
    "        # dropout layer (p=0.2)\n",
    "        # dropout prevents overfitting of data\n",
    "        self.droput = nn.Dropout(0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.elu = nn.ELU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #x is the input tensor\n",
    "        out = self.fc1(x)\n",
    "        #add hidden layer, with relu activation function\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        # add hidden layer, with relu activation function\n",
    "        out = self.relu(out)\n",
    "        # add dropout instead of relu or not..?\n",
    "        #out = self.droput(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize UNSW_NB15 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsw_nb15_training = UNSW_NB15(file_path ='UNSW_NB15_training-set.csv')\n",
    "training = unsw_nb15_training.get_dataframe()\n",
    "x_training = training.iloc[:, 0:-1].values\n",
    "y_training = training.iloc[:, -1].values # Last two/1 ? columns are categories and labels\n",
    "\n",
    "\n",
    "unsw_nb15_testing = UNSW_NB15(file_path ='UNSW_NB15_testing-set.csv')\n",
    "testing = unsw_nb15_testing.get_dataframe()\n",
    "x_testing = testing.iloc[:, 0:-1].values\n",
    "y_testing = testing.iloc[:, -1].values # Last two/1 ? columns are categories and labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some parameters first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 43\n",
    "hidden_size = 64      # 1st layer number of neurons\n",
    "hidden_size_2 = 64    # 2nd layer number of neurons\n",
    "num_classes = 10      # There are 9 different types of malicious packets + Normal\n",
    "\n",
    "num_epochs = 40\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "n_total_steps = len(x_training)\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Neural Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=43, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (droput): Dropout(p=0.2)\n",
      "  (relu): ReLU()\n",
      "  (elu): ELU(alpha=1.0)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size, hidden_size, hidden_size_2, num_classes).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss and optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/40], Step [175329/175341], Loss: 0.0527\n",
      "Epoch [20/40], Step [175329/175341], Loss: 0.0519\n",
      "Epoch [30/40], Step [175329/175341], Loss: 0.0528\n",
      "Epoch [40/40], Step [175329/175341], Loss: 0.0528\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "   \n",
    "    for i in range(0, x_training.shape[0], batch_size):\n",
    "\n",
    "\n",
    "        x = torch.as_tensor(x_training[i:i+batch_size], dtype=torch.float).to(device)\n",
    "        y = torch.as_tensor(y_training[i:i+batch_size], dtype=torch.long).to(device)\n",
    "        \n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network: 55.134636172616084 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "x_testing_vals= x_testing\n",
    "y_testing_vals = y_testing\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0  \n",
    "    for i in range(0, x_testing_vals.shape[0], batch_size):\n",
    "        x = torch.as_tensor(x_testing_vals[i:i+batch_size], dtype=torch.float).to(device)\n",
    "        y = torch.as_tensor(y_testing_vals[i:i+batch_size], dtype=torch.long).to(device)\n",
    "        \n",
    "        outputs = model(x)\n",
    "        if len(outputs.data) > 0:\n",
    "            # max returns (value ,index)\n",
    "            _, predicted = torch.max(outputs.data, dim=1)\n",
    "            n_samples += y.size(0)\n",
    "            n_correct += (predicted == y).sum().item()\n",
    "        else:\n",
    "            print(\"what???\")\n",
    "            print(x, outputs.data)\n",
    "    acc = 100.0 * n_correct / (n_samples+1)\n",
    "    print(f'Accuracy of the network: {acc} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try with an auto-encoder ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
