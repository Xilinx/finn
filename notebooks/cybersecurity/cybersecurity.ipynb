{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "#needed to create the Neural Network\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#needed to preprocess the dataset\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from dataloader import UNSW_NB15\n",
    "\n",
    "#general\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspired by [this github file](https://github.com/alik604/cyber-security/blob/master/Intrusion-Detection/UNSW_NB15%20-%20Torch%20MLP%20and%20autoEncoder.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get UNSW_NB15 train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/a%20part%20of%20training%20and%20testing%20set/UNSW_NB15_training-set.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/a%20part%20of%20training%20and%20testing%20set/UNSW_NB15_testing-set.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Neural Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define NN architecture\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(50, 100)\n",
    "        self.prelu = nn.PReLU(1)\n",
    "        self.out = nn.Linear(100, 1)\n",
    "        self.out_act = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        a1 = self.fc1(input_)\n",
    "        h1 = self.relu1(a1)\n",
    "        dout = self.dout(h1)\n",
    "        a2 = self.fc2(dout)\n",
    "        h2 = self.prelu(a2)\n",
    "        a3 = self.out(h2)\n",
    "        y = self.out_act(a3)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Train,   Test   and    Display_Loss_Plot    methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer):\n",
    "    \n",
    "    loss_history = list()\n",
    "    model.train()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, target ( or labels)]\n",
    "        inputs , target = data\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #MOVING THE TENSORS TO THE CONFIGURED DEVICE\n",
    "        #inputs, target = inputs.to(device), target.to(device)\n",
    "        \n",
    "        #FORWARD PASS\n",
    "        output = model(inputs.float())\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        #BACKWARD AND OPTIMIZE\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # PREDICTIONS\n",
    "        pred = np.round(output.detach().numpy())\n",
    "        target = target.float()\n",
    "        y_true.extend(target.tolist()) \n",
    "        y_pred.extend(pred.reshape(-1).tolist())\n",
    "        \n",
    "        loss_history.append(loss.item())     \n",
    "    print(\"Accuracy on training set is\" , accuracy_score(y_true,y_pred))\n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING THE MODEL\n",
    "def test(model, device, test_loader):\n",
    "    #model in eval mode skips Dropout etc\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    # set the requires_grad flag to false as we are in the test mode\n",
    "    with torch.no_grad():\n",
    "        for i in test_loader:\n",
    "            \n",
    "            #LOAD THE DATA IN A BATCH\n",
    "            data,target = i\n",
    "            \n",
    "            # moving the tensors to the configured device\n",
    "            #data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # the model on the data\n",
    "            output = model(data.float())\n",
    "                       \n",
    "            #PREDICTIONS\n",
    "            pred = np.round(output)\n",
    "            target = target.float()\n",
    "            y_true.extend(target.tolist()) \n",
    "            y_pred.extend(pred.reshape(-1).tolist())\n",
    "    \n",
    "            \n",
    "    print(\"Accuracy on test set is\" , accuracy_score(y_true,y_pred))\n",
    "    print(\"***********************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_loss_plot(losses):\n",
    "    x_axis = [i for i in range(len(losses))]\n",
    "    plt.plot(x_axis,losses)\n",
    "    plt.title('Loss of the model')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('Cross entropy loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some parameters first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 56      # 42 for integer encoding 196\n",
    "hidden_size = 100      # 1st layer number of neurons\n",
    "hidden_size_2 = 100    # 2nd layer number of neurons\n",
    "num_classes = 1    # binary classification\n",
    "\n",
    "num_epochs = 80\n",
    "BATCH_SIZE_1 = 5000 #train_loader as it has  175341  observations\n",
    "BATCH_SIZE_2 = 5000 #test_loader as it has  82332  observations\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize UNSW_NB15 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([175341, 57])\n",
      "torch.Size([82332, 57])\n"
     ]
    }
   ],
   "source": [
    "#get the train dataframe\n",
    "train_dataset = UNSW_NB15(file_path ='UNSW_NB15_training-set.csv')\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE_1, shuffle=not False)\n",
    "\n",
    "#get the test dataframe\n",
    "test_dataset = UNSW_NB15(file_path ='UNSW_NB15_testing-set.csv')\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE_2, shuffle=not False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Neural Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=56, out_features=50, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (dout): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (prelu): PReLU(num_parameters=1)\n",
      "  (out): Linear(in_features=100, out_features=1, bias=True)\n",
      "  (out_act): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_size)\n",
    "#model = Net(input_size, hidden_size, num_classes)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss and optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets Train, Test the model and see the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a3de8df3d4143608293a99b8da75b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set is 0.328502746077643\n",
      "Accuracy on training set is 0.521395452290109\n",
      "Accuracy on training set is 0.7292475804289926\n",
      "Accuracy on training set is 0.748706805595953\n",
      "Accuracy on training set is 0.7397471213235923\n",
      "Accuracy on training set is 0.7447259910688316\n",
      "Accuracy on training set is 0.7567083568589206\n",
      "Accuracy on training set is 0.7565885902327465\n",
      "Accuracy on training set is 0.7573357058531661\n",
      "Accuracy on training set is 0.7571075789461678\n",
      "Accuracy on training set is 0.7581341500276604\n",
      "Accuracy on training set is 0.7577862564944878\n",
      "Accuracy on training set is 0.7575296137241148\n",
      "Accuracy on training set is 0.75748969151539\n",
      "Accuracy on training set is 0.7588128275759806\n",
      "Accuracy on training set is 0.7568281234850948\n",
      "Accuracy on training set is 0.7585276689422326\n",
      "Accuracy on training set is 0.758459230870133\n",
      "Accuracy on training set is 0.7596112717504748\n",
      "Accuracy on training set is 0.7603469810255445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "running_loss = []\n",
    "num_epochs=20\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "        loss_epoch = train(model, device, train_loader, optimizer)\n",
    "        running_loss.append(loss_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training and testing, showing the loss now...\n"
     ]
    }
   ],
   "source": [
    "print(\"Done training and testing, showing the loss now...\")    \n",
    "loss_per_epoch = [np.mean(loss_per_epoch) for loss_per_epoch in running_loss]\n",
    "display_loss_plot(loss_per_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[66.64645703633626,\n",
       " 47.05401500066122,\n",
       " 26.82444339328342,\n",
       " 25.01007541020711,\n",
       " 25.82344431347317,\n",
       " 25.382208506266277,\n",
       " 24.277635097503662,\n",
       " 24.160190211402046,\n",
       " 24.233546045091416,\n",
       " 24.33760478761461,\n",
       " 24.09851524564955,\n",
       " 24.151273992326523,\n",
       " 24.14856831232707,\n",
       " 24.192487451765274,\n",
       " 23.99700551562839,\n",
       " 24.318423218197292,\n",
       " 24.116024017333984,\n",
       " 24.049747149149578,\n",
       " 23.991874906751846,\n",
       " 23.976964314778645]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set is 0.6175484623232765\n",
      "***********************************************************\n"
     ]
    }
   ],
   "source": [
    "test(model,device,test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is the model trainable?\n",
    "\n",
    "Lets check the performance of a Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train = train_loader.dataset.dataframe.drop(columns='label')\n",
    "y_train = train_loader.dataset.dataframe.label\n",
    "X_test = test_loader.dataset.dataframe.drop(columns='label')\n",
    "y_test = test_loader.dataset.dataframe.label\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "preds = lr.predict(X_test)\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "    70% accuracy in test set with Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets try to see which categories matter, therefore should enter the 1hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(data=train_loader.dataset.dataframe, x = \"proto\", hue=\"label\")\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1 = sns.countplot(data=train_loader.dataset.dataframe, x = \"state\", hue=\"label\")\n",
    "ax1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2 = sns.countplot(data=train_loader.dataset.dataframe, x = \"service\", hue=\"label\")\n",
    "ax2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recreate this paper's results\n",
    "    https://www.researchgate.net/publication/332100759_Intrusion_Detection_Using_Big_Data_and_Deep_Learning_Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
