{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#!pip install --user pandas\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "\n",
    "from dataloader import UNSW_NB15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspired by [this github file](https://github.com/alik604/cyber-security/blob/master/Intrusion-Detection/UNSW_NB15%20-%20Torch%20MLP%20and%20autoEncoder.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get UNSW_NB15 train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/a%20part%20of%20training%20and%20testing%20set/UNSW_NB15_training-set.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/a%20part%20of%20training%20and%20testing%20set/UNSW_NB15_testing-set.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Neural Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define NN architecture\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size,hidden1, hidden2, hidden3, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(hidden1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(hidden2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden2, hidden3)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(hidden3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        \n",
    "        self.out = nn.Linear(hidden3, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        a1 = self.fc1(x)\n",
    "        #b1 = self.batchnorm1(a1)\n",
    "        h1 = self.relu1(a1)\n",
    "        dout1 = self.dout(h1)\n",
    "        \n",
    "        a2 = self.fc2(dout1)\n",
    "        #b2 = self.batchnorm2(a2)\n",
    "        h2 = self.relu2(a2)\n",
    "        dout2 = self.dout(h2)\n",
    "        \n",
    "        a3 = self.fc3(dout2)\n",
    "        #b3 = self.batchnorm3(a3)\n",
    "        h3 = self.relu3(a3)\n",
    "        dout3 = self.dout(h3)\n",
    "        \n",
    "        a4= self.out(dout3)\n",
    "        y = self.sigmoid(a4)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Train,   Test   and    Display_Loss_Plot    methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, target ( or labels)]\n",
    "        inputs , target = data\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #MOVING THE TENSORS TO THE CONFIGURED DEVICE\n",
    "        #inputs, target = inputs.to(device), target.to(device)\n",
    "        \n",
    "        #FORWARD PASS\n",
    "        output = model(inputs.float())\n",
    "\n",
    "        loss = criterion(output, target.unsqueeze(1))\n",
    "        #import pdb; pdb.set_trace()\n",
    "        \n",
    "        #BACKWARD AND OPTIMIZE\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # PREDICTIONS\n",
    "        #pred = np.round(output.detach().numpy())\n",
    "        pred = output.detach().numpy() > 0.5  \n",
    "        target = target.float()\n",
    "        y_true.extend(target.tolist()) \n",
    "        y_pred.extend(pred.reshape(-1).tolist())\n",
    "        \n",
    "        losses.append(loss.data.numpy()) \n",
    "    #print(\"Accuracy on training set is\" , accuracy_score(y_true,y_pred))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING THE MODEL\n",
    "def test(model, device, test_loader):\n",
    "    #model in eval mode skips Dropout etc\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    # set the requires_grad flag to false as we are in the test mode\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            \n",
    "            #LOAD THE DATA IN A BATCH\n",
    "            inputs ,target = data\n",
    "            \n",
    "            # the model on the data\n",
    "            output = model(inputs.float())\n",
    "                       \n",
    "            #PREDICTIONS\n",
    "            pred = np.round(output)\n",
    "            #pred = output.detach().numpy() > 0.5 \n",
    "            pred = pred * 1\n",
    "            target = target.float()\n",
    "            y_true.extend(target.tolist()) \n",
    "            y_pred.extend(pred.reshape(-1).tolist())\n",
    "        \n",
    "    return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_loss_plot(losses):\n",
    "    x_axis = [i for i in range(len(losses))]\n",
    "    plt.plot(x_axis,losses)\n",
    "    plt.title('Loss of the model')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('Cross entropy loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define some parameters first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "input_size = 196      # 42 for integer encoding 196\n",
    "hidden1 = 128      # 1st layer number of neurons\n",
    "hidden2 = 64\n",
    "hidden3 = 32\n",
    "num_classes = 1    # binary classification\n",
    "\n",
    "num_epochs = 1000  #500 1000 100 100\n",
    "batch_size = 500   #100 1000 100 500  \n",
    "lr = 0.01          #0.01 0.01 0.005 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Neural Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(input_size, hidden1, hidden2, hidden3, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define loss and optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize UNSW_NB15 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are not slitted into validation and train set\n",
    "train_dataset = UNSW_NB15(file_path ='UNSW_NB15_training-set.csv')\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#get the test dataframe\n",
    "test_dataset = UNSW_NB15(file_path ='UNSW_NB15_testing-set.csv')\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Train, Test the model and see the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "running_loss = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "        loss_epoch = train(model, device, train_loader, optimizer,criterion)\n",
    "        running_loss.append(loss_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model,device,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_per_epoch = [np.mean(loss_per_epoch) for loss_per_epoch in running_loss]\n",
    "display_loss_plot(loss_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**********************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    num_epochs = 200\n",
    "    batch_size = 8000 \n",
    "    lr = 0.001\n",
    "    accuracy on test set = 0.7572389836272653\n",
    "<img src=\"data/loss_function_75.723898_acc.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quntization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('UNSW_NB15_training-set.csv')\n",
    "test = pd.read_csv('UNSW_NB15_testing-set.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "def integer_encoding(df_):\n",
    "    df = df_.copy()\n",
    "    \"\"\"Applies integer encoding to the object columns of the dataframe\"\"\"\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    \n",
    "    for column in df.select_dtypes('object').columns.tolist():\n",
    "        df[column] = le.fit_transform(df[column])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from here https://stackoverflow.com/questions/51471097/vectorized-conversion-of-decimal-integer-array-to-binary-array-in-numpy\n",
    "def dec2bin(column: pd.Series, number_of_bits: int, left_msb:bool= True )-> pd.Series: \n",
    "    \"\"\"Convert a decimal pd.Series to binary pd.Series with numbers in their base-2 equivalents.\n",
    "    The output is a numpy nd array.   \n",
    "    # adapted from here https://stackoverflow.com/questions/51471097/vectorized-conversion-of-decimal-integer-array-to-binary-array-in-numpy\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "     column: pd.Series\n",
    "        Series wit all decimal numbers that will be cast to binary\n",
    "     number_of_bits: str\n",
    "        The desired number of bits for the binary number. If bigger than what is needed then those bits will be 0.\n",
    "        The number_of_bits should be >= than what is needed to express the largest decimal input \n",
    "     left_msb: bool\n",
    "        Specify that the most significant digit is the leftmost element. If this is False, it will be the rightmost element.\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "       Numpy array with all elements in binary representation of the input.\n",
    "        \n",
    "    \"\"\"\n",
    " \n",
    "    my_binary_repr = lambda number, nbits:  np.binary_repr(number, nbits)[::-1]\n",
    "    func = my_binary_repr if left_msb else np.binary_repr\n",
    "    \n",
    "    return np.vectorize(func)(column.values, number_of_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from here https://stackoverflow.com/questions/51471097/vectorized-conversion-of-decimal-integer-array-to-binary-array-in-numpy\n",
    "def dec2bin(column: pd.Series, number_of_bits: int, left_msb:bool= True )-> pd.Series: \n",
    "    \"\"\"Convert a decimal pd.Series to binary pd.Series with numbers in their base-2 equivalents.\n",
    "    The output is a numpy nd array.   \n",
    "    # adapted from here https://stackoverflow.com/questions/51471097/vectorized-conversion-of-decimal-integer-array-to-binary-array-in-numpy\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "     column: pd.Series\n",
    "        Series wit all decimal numbers that will be cast to binary\n",
    "     number_of_bits: str\n",
    "        The desired number of bits for the binary number. If bigger than what is needed then those bits will be 0.\n",
    "        The number_of_bits should be >= than what is needed to express the largest decimal input \n",
    "     left_msb: bool\n",
    "        Specify that the most significant digit is the leftmost element. If this is False, it will be the rightmost element.\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "       Numpy array with all elements in binary representation of the input.\n",
    "        \n",
    "    \"\"\"\n",
    " \n",
    "    my_binary_repr_left_most = lambda number, nbits:  np.char.join(\" \", np.binary_repr(number, nbits)[::-1]) \n",
    "    my_binary_repr_right_most = lambda number, nbits:  np.char.join(\" \", np.binary_repr(number, nbits))\n",
    "    func = my_binary_repr if left_msb else my_binary_repr_right_most\n",
    "    \n",
    "    return np.vectorize(func)(column.values, number_of_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([train,test])\n",
    "#skip_cols = ['id','attack_cat'] #this is what they have\n",
    "skip_cols = ['id', 'attack_cat'] #this might need to change\n",
    "\n",
    "binary_matrix = None # final matrix of bit vetors\n",
    "first_iteration = True\n",
    "\n",
    "# gets the smallest positive number of a vector\n",
    "get_min_positive_number = lambda vector: vector[vector > 0].min()\n",
    "# computes the minimum required bits to represent eachs number from a vector of numbers\n",
    "get_min_bits = lambda vector: math.ceil(math.log2(float(df[column].max())+1))\n",
    "\n",
    "df = integer_encoding(df) #perform integer encoding on the string columns just as they do\n",
    "\n",
    "for column in df.columns:\n",
    "    \n",
    "    if column not in skip_cols:\n",
    "        m = get_min_positive_number(df[column])\n",
    "        m = 1/m\n",
    "\n",
    "        if m>1:\n",
    "            df[column] = df[column] *m\n",
    "\n",
    "        maxbits = get_min_bits(df[column])\n",
    "\n",
    "        binary_vector = dec2bin(df[column].astype(np.uint32), maxbits, left_msb=False).reshape((-1,1))\n",
    "        if first_iteration:\n",
    "            binary_matrix = binary_vector\n",
    "            first_iteration = False\n",
    "        else:\n",
    "            binary_matrix = np.hstack([binary_matrix, binary_vector])\n",
    "            \n",
    "            \n",
    "id_ = round((2*binary_matrix.shape[0])/3)\n",
    "id6 = round((2*binary_matrix.shape[0])/3/6)\n",
    "\n",
    "infeat_train = binary_matrix[0:id_-id6,:]\n",
    "infeat_valid = binary_matrix[(id_-id6+1):id_,:]\n",
    "infeat_test  = binary_matrix[id_+1:,:]\n",
    "\n",
    "infeat_train = infeat_train[1:round(infeat_train.shape[0]/10),:];\n",
    "infeat_valid = infeat_valid[1:round(infeat_valid.shape[0]/10),:];\n",
    "infeat_test  = infeat_test[1:round(infeat_test.shape[0]/10),:]\n",
    "\n",
    "#np.savetxt(\"binary_dataset_no_sep.csv\", binary_matrix, fmt=\"%s\",delimiter=\"\")\n",
    "\n",
    "np.savetxt(\"fds_unswb15_train.txt\", binary_matrix, fmt=\"%s\",delimiter=\" \")\n",
    "np.savetxt('fds_unswb15_valid.txt', binary_matrix, fmt=\"%s\",delimiter=\" \")\n",
    "np.savetxt('fds_unswb15_test.txt', binary_matrix, fmt=\"%s\",delimiter=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 1 1 0',\n",
       "       '0 1 1 1 0 0 0 1', '0 0 0 0', '0 1 0 0',\n",
       "       '0 0 0 0 0 0 0 0 0 0 0 1 1 0', '0 0 0 0 0 0 0 0 0 0 0 1 0 0',\n",
       "       '0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0',\n",
       "       '0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0',\n",
       "       '0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 1 0',\n",
       "       '1 1 1 1 1 1 0 0', '1 1 1 1 1 1 1 0',\n",
       "       '0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0 0',\n",
       "       '0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1',\n",
       "       '0 0 0 0 0 0 0 0 0 0 0 0 0', '0 0 0 0 0 0 0 0 0 0 0 0 0',\n",
       "       '0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1',\n",
       "       '0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1',\n",
       "       '0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1',\n",
       "       '0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 1',\n",
       "       '1 1 1 1 1 1 1 1',\n",
       "       '0 0 1 0 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 0 0',\n",
       "       '1 0 0 0 0 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1',\n",
       "       '1 1 1 1 1 1 1 1', '0 0 0 0 0 0 0 0 0 0 0 0 0 0',\n",
       "       '0 0 0 0 0 0 0 0 0 0 0 0 0', '0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0',\n",
       "       '0 0 0 0 0 1 0 1 0 1 1', '0 0 0 0 0 1 0 1 0 1 1',\n",
       "       '0 0 0 0 0 0 0 0', '0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0',\n",
       "       '0 0 0 0 0 1', '0 0 0', '0 0 0 0 0 1', '0 0 0 0 0 1',\n",
       "       '0 0 0 0 0 1', '0 0 0 0 0 0 1', '0 0 0', '0 0 0', '0 0 0 0 0',\n",
       "       '0 0 0 0 0 1', '0 0 0 0 0 1', '0', '0'], dtype='<U268')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_matrix[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(257673, 43)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differences between matlab script and what I have:\n",
    "    train set and dataset order,i did not switch. But they do shuffle everything so doesn't matter.\n",
    "    put space between everything takes up a lot of time\n",
    "    files are different because they shuffle everything\n",
    "    I dropped the attack cat and the id columns, like they did, not sure if label also need to be dropped\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#string with 2 names\n",
    "s = [\"UNSW_NB15_testing-set.csv\", \"UNSW_NB15_training-set.csv\"]; #The names are the other way around\n",
    "\n",
    "opts = detectImportOptions(char(s(1)));\n",
    "T0 = readtable(char(s(1)),opts);\n",
    "opts = detectImportOptions(char(s(2)));\n",
    "T1 = readtable(char(s(2)),opts);\n",
    "T = [T0; T1];\n",
    "#T is a dtaframe with both test set and train set\n",
    "binfeat = [];\n",
    "for i = 1:size(T,2)\n",
    "    if i == 1 || i == (size(T,2)-1) ## for columns \"id\" and \"attack_cat\" do nothing\n",
    "        continue;\n",
    "    end\n",
    "    \n",
    "    M = table2array(T(:,i)); # M is column i (=/id =/attack_cat)\n",
    "    \n",
    "    if iscell(M)  #check if M is a cell array, a column is not a cell array. Cell array = {1} or {[1]}\n",
    "        ue = string(unique(M));\n",
    "        for j = 1:length(ue)\n",
    "            new_M(strcmp(string(M),(ue(j)))) = j-1;\n",
    "        end\n",
    "        M = new_M;\n",
    "    end\n",
    "    \n",
    "    m = min(M(M~=0)); #m= minimum value from column M, not counting with 0s\n",
    "    m = 1/m;    \n",
    "    if m > 1\n",
    "        M = M*m; #multiply all column values (cells) by 100000 if m=0.000001..\n",
    "    end\n",
    "    \n",
    "    #max(M): maiximum number of column (59999989)\n",
    "    #double():cast to doub\n",
    "    # le  (599999989)\n",
    "    #log2():logarithm of (59999999990) (+1 dont forget) =25.58485\n",
    "    #ceil(): arredondar p cima = 26 = b\n",
    "    #de2bi():convert to binary the decimal column uint32(M), specify that the most significant digit is the leftmost and set the\n",
    "    #desired number of columns to b, \n",
    "    b = ceil(log2(double(max(M))+1));#\n",
    "    feat = dec2bin(uint32(M),b); \n",
    "    binfeat = [binfeat feat];\n",
    "end\n",
    "\n",
    "binfeat = binfeat(randperm(length(binfeat)),:); # to shuffle everything\n",
    "pause(2.0);\n",
    "id  = round(2*length(binfeat)/3);\n",
    "id6 = round(2*length(binfeat)/3/6);\n",
    "infeat_train = binfeat(1:id-id6,:);\n",
    "infeat_valid = binfeat((id-id6+1):id,:);\n",
    "infeat_test  = binfeat(id+1:end,:);\n",
    "\n",
    "infeat_train = infeat_train(1:round(size(infeat_train,1)/10),:);\n",
    "infeat_valid = infeat_valid(1:round(size(infeat_valid,1)/10),:);\n",
    "infeat_test  = infeat_test(1:round(size(infeat_test,1)/10),:);\n",
    "\n",
    "dataset_size = [length(infeat_train)+length(infeat_valid)+length(infeat_test) length(infeat_train) length(infeat_valid) length(infeat_test)]\n",
    "\n",
    "fileID = fopen('fds_unswb15_train.txt','w');\n",
    "for i = 1:size(infeat_train,1)\n",
    "    i/size(infeat_train,1)\n",
    "    for j = 1:size(infeat_train,2)\n",
    "        fprintf(fileID,'#d ',infeat_train(i,j));\n",
    "    end\n",
    "    fprintf(fileID,\"\\n\");\n",
    "end\n",
    "fclose(fileID);\n",
    "\n",
    "fileID = fopen('fds_unswb15_valid.txt','w');\n",
    "for i = 1:size(infeat_valid,1)\n",
    "    i/size(infeat_valid,1)\n",
    "    for j = 1:size(infeat_valid,2)\n",
    "        fprintf(fileID,'#d ',infeat_valid(i,j));\n",
    "    end\n",
    "    fprintf(fileID,\"\\n\");\n",
    "end\n",
    "fclose(fileID);\n",
    "\n",
    "fileID = fopen('fds_unswb15_test.txt','w');\n",
    "for i = 1:size(infeat_test,1)\n",
    "    i/size(infeat_test,1)\n",
    "    for j = 1:size(infeat_test,2)\n",
    "        fprintf(fileID,'#d ',infeat_test(i,j));\n",
    "    end\n",
    "    fprintf(fileID,\"\\n\");\n",
    "end\n",
    "fclose(fileID);\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
