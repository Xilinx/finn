{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#!pip install --user pandas\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "\n",
    "from brevitas.nn import QuantIdentity, QuantConv2d, QuantReLU\n",
    "from brevitas.core.quant import QuantType\n",
    "\n",
    "from dataloader import UNSW_NB15\n",
    "from dataloader_quantized import UNSW_NB15_quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspired by [this github file](https://github.com/alik604/cyber-security/blob/master/Intrusion-Detection/UNSW_NB15%20-%20Torch%20MLP%20and%20autoEncoder.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get UNSW_NB15 train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/a%20part%20of%20training%20and%20testing%20set/UNSW_NB15_training-set.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/a%20part%20of%20training%20and%20testing%20set/UNSW_NB15_testing-set.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Neural Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantLeNet(Module):\n",
    "    def __init__(self, input_size,hidden1, hidden2, hidden3, num_classes):\n",
    "        super(QuantLeNet, self).__init__()\n",
    "        self.fc1   = QuantLinear(input_size, hidden1, bias=True, weight_bit_width=8)\n",
    "        self.relu1 = QuantReLU(bit_width=8)\n",
    "        self.fc2   = QuantLinear(hidden1, hidden2, bias=True, weight_bit_width=8)\n",
    "        self.relu2 = QuantReLU(bit_width=8)\n",
    "        self.fc3   = QuantLinear(hidden2, hidden3, bias=True, weight_bit_width=8)\n",
    "        self.relu3 = QuantReLU(bit_width=8)\n",
    "        self.out   = QuantLinear(hidden3, num_classes, bias=False, weight_bit_width=8)\n",
    "        self.out_activ_quantized = QuantIdentity(bit_width=8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.out(out)\n",
    "        out = self.out_activ_quantized(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Train,   Test   and    Display_Loss_Plot    methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, target ( or labels)]\n",
    "        inputs , target = data\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #MOVING THE TENSORS TO THE CONFIGURED DEVICE\n",
    "        #inputs, target = inputs.to(device), target.to(device)\n",
    "        \n",
    "        #FORWARD PASS\n",
    "        output = model(inputs.float())\n",
    "\n",
    "        loss = criterion(output, target.unsqueeze(1))\n",
    "        #import pdb; pdb.set_trace()\n",
    "        \n",
    "        #BACKWARD AND OPTIMIZE\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # PREDICTIONS\n",
    "        #pred = np.round(output.detach().numpy())\n",
    "        pred = output.detach().numpy() > 0.5  \n",
    "        target = target.float()\n",
    "        y_true.extend(target.tolist()) \n",
    "        y_pred.extend(pred.reshape(-1).tolist())\n",
    "        \n",
    "        losses.append(loss.data.numpy()) \n",
    "    #print(\"Accuracy on training set is\" , accuracy_score(y_true,y_pred))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING THE MODEL\n",
    "def test(model, device, test_loader):\n",
    "    #model in eval mode skips Dropout etc\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    # set the requires_grad flag to false as we are in the test mode\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            \n",
    "            #LOAD THE DATA IN A BATCH\n",
    "            inputs ,target = data\n",
    "            \n",
    "            # the model on the data\n",
    "            output = model(inputs.float())\n",
    "                       \n",
    "            #PREDICTIONS\n",
    "            pred = np.round(output)\n",
    "            #pred = output.detach().numpy() > 0.5 \n",
    "            pred = pred * 1\n",
    "            target = target.float()\n",
    "            y_true.extend(target.tolist()) \n",
    "            y_pred.extend(pred.reshape(-1).tolist())\n",
    "        \n",
    "    return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_loss_plot(losses):\n",
    "    x_axis = [i for i in range(len(losses))]\n",
    "    plt.plot(x_axis,losses)\n",
    "    plt.title('Loss of the model')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('Cross entropy loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define some parameters first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "input_size = 196      # 42 for integer encoding 196\n",
    "hidden1 = 128      # 1st layer number of neurons\n",
    "hidden2 = 64\n",
    "hidden3 = 32\n",
    "num_classes = 1    # binary classification\n",
    "\n",
    "num_epochs = 200\n",
    "batch_size = 5000 \n",
    "lr = 0.001        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Neural Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QuantLeNet(input_size, hidden1, hidden2, hidden3, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define loss and optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize UNSW_NB15 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([175341, 197])\n",
      "torch.Size([82332, 197])\n",
      "torch.Size([175341, 197])\n",
      "torch.Size([82332, 197])\n"
     ]
    }
   ],
   "source": [
    "#these are not slitted into validation and train set\n",
    "train_dataset = UNSW_NB15(file_path ='data/UNSW_NB15_training-set.csv')\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#get the test dataframe\n",
    "test_dataset = UNSW_NB15(file_path ='data/UNSW_NB15_testing-set.csv')\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#Get the Quantized versions\n",
    "train_quantized_dataset = UNSW_NB15_quantized(file_path_train='data/UNSW_NB15_training-set.csv', \\\n",
    "                                              file_path_test = \"data/UNSW_NB15_testing-set.csv\", \\\n",
    "                                              train=True)\n",
    "train_quantized_loader = DataLoader(train_quantized_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_quantized_dataset = UNSW_NB15_quantized(file_path_train='data/UNSW_NB15_training-set.csv', \\\n",
    "                                              file_path_test = \"data/UNSW_NB15_testing-set.csv\", \\\n",
    "                                              train=False)\n",
    "test_quantized_loader = DataLoader(test_quantized_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Train, Test the model and see the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_quantized_loader\n",
    "test_loader = test_quantized_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f81568c05c245768738a17ab3fe52b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "running_loss = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "        loss_epoch = train(model, device, train_loader, optimizer,criterion)\n",
    "        running_loss.append(loss_epoch)\n",
    "#Save the model!!\n",
    "torch.save(model.state_dict(), \"MLP_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load(\"MLP_model\"))\n",
    "test(model,device,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_per_epoch = [np.mean(loss_per_epoch) for loss_per_epoch in running_loss]\n",
    "display_loss_plot(loss_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**********************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    num_epochs = 200\n",
    "    batch_size = 8000 \n",
    "    lr = 0.001\n",
    "    accuracy on test set = 0.7235218384103387\n",
    "<img src=\"data/train_not_quantized_72.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
