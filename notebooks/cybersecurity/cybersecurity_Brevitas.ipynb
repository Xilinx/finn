{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4d/51/bafcff417cd857bc6684336320863b5e5af280530213ef8f534b6042cfe6/pandas-1.1.4-cp36-cp36m-manylinux1_x86_64.whl (9.5MB)\n",
      "\u001b[K     |################################| 9.5MB 2.1MB/s eta 0:00:01     |#######################         | 6.8MB 2.1MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /opt/conda/lib/python3.6/site-packages (from pandas) (1.19.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.1.4\n",
      "Collecting scikit-learn\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/a1/273def87037a7fb010512bbc5901c31cfddfca8080bc63b42b26e3cc55b3/scikit_learn-0.23.2-cp36-cp36m-manylinux1_x86_64.whl (6.8MB)\n",
      "\u001b[K     |################################| 6.8MB 4.5MB/s eta 0:00:01     |#########                       | 2.1MB 183kB/s eta 0:00:26     |##############                  | 3.2MB 183kB/s eta 0:00:20     |############################    | 6.1MB 4.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib>=0.11 (from scikit-learn)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/c9/f58220ac44a1592f79a343caba12f6837f9e0c04c196176a3d66338e1ea8/joblib-0.17.0-py3-none-any.whl (301kB)\n",
      "\u001b[K     |################################| 307kB 10.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.6/site-packages (from scikit-learn) (1.19.4)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.6/site-packages (from scikit-learn) (1.5.2)\n",
      "Installing collected packages: joblib, threadpoolctl, scikit-learn\n",
      "Successfully installed joblib-0.17.0 scikit-learn-0.23.2 threadpoolctl-2.1.0\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (4.31.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user pandas\n",
    "!pip install --user scikit-learn\n",
    "!pip install tqdm\n",
    "\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "from brevitas.nn import QuantIdentity, QuantConv2d, QuantReLU, QuantLinear, QuantHardTanh\n",
    "from brevitas.core.quant import QuantType\n",
    "\n",
    "from dataloader import UNSW_NB15\n",
    "from dataloader_quantized import UNSW_NB15_quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspired by [this github file](https://github.com/alik604/cyber-security/blob/master/Intrusion-Detection/UNSW_NB15%20-%20Torch%20MLP%20and%20autoEncoder.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get UNSW_NB15 train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/a%20part%20of%20training%20and%20testing%20set/UNSW_NB15_training-set.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/a%20part%20of%20training%20and%20testing%20set/UNSW_NB15_testing-set.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Neural Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Train,   Test   and    Display_Loss_Plot    methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):        \n",
    "        # get the inputs; data is a list of [inputs, target ( or labels)]\n",
    "        inputs , target = data\n",
    "        optimizer.zero_grad()   \n",
    "        \n",
    "        #FORWARD PASS\n",
    "        output = model(inputs.float())\n",
    "        print(\"Check if output has nans=\",torch.isnan(output).any())\n",
    "\n",
    "        loss = criterion(output, target.unsqueeze(1))\n",
    "        \n",
    "        #BACKWARD AND OPTIMIZE        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # PREDICTIONS\n",
    "        #pred = np.round(output.detach().numpy())\n",
    "        pred = output.detach().numpy() > 0.5  \n",
    "        target = target.float()\n",
    "        y_true.extend(target.tolist()) \n",
    "        y_pred.extend(pred.reshape(-1).tolist())\n",
    "        \n",
    "        losses.append(loss.data.numpy()) \n",
    "    #print(\"Accuracy on training set is\" , accuracy_score(y_true,y_pred))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING THE MODEL\n",
    "def test(model, device, test_loader):    \n",
    "    model.eval()   #model in eval mode skips Dropout etc\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "   \n",
    "    with torch.no_grad(): # set the requires_grad flag to false as we are in the test mode\n",
    "        for data in test_loader:\n",
    "            \n",
    "            #LOAD THE DATA IN A BATCH\n",
    "            inputs ,target = data\n",
    "            \n",
    "            # the model on the data\n",
    "            output = model(inputs.float())\n",
    "            print(\"Check if output has nans=\",torch.isnan(output).any())\n",
    "\n",
    "            output = nn.Sigmoid(output)  \n",
    "            \n",
    "            #PREDICTIONS\n",
    "            pred = np.round(output)\n",
    "            #pred = output.detach().numpy() > 0.5 \n",
    "            pred = pred * 1\n",
    "            target = target.float()\n",
    "            y_true.extend(target.tolist()) \n",
    "            y_pred.extend(pred.reshape(-1).tolist())\n",
    "        \n",
    "    return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_loss_plot(losses):\n",
    "    x_axis = [i for i in range(len(losses))]\n",
    "    plt.plot(x_axis,losses)\n",
    "    plt.title('Loss of the model')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('Cross entropy loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define some parameters first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "input_size = 196      # 42 for integer encoding 196\n",
    "hidden1 = 128      # 1st layer number of neurons\n",
    "hidden2 = 64\n",
    "hidden3 = 32\n",
    "num_classes = 1    # binary classification\n",
    "\n",
    "num_epochs = 2\n",
    "batch_size = 5000 \n",
    "lr = 0.001        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Neural Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantLeNet(nn.Module):\n",
    "    def __init__(self, input_size,hidden1, hidden2, hidden3, num_classes):\n",
    "        super(QuantLeNet, self).__init__()\n",
    "        self.fc1   = QuantLinear(input_size, hidden1, bias=True, weight_bit_width=8)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(hidden1)\n",
    "        self.relu1 = QuantReLU(bit_width=8)\n",
    "        \n",
    "        self.fc2   = QuantLinear(hidden1, hidden2, bias=True, weight_bit_width=8)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(hidden2)\n",
    "        self.relu2 = QuantReLU(bit_width=8)\n",
    "        \n",
    "        self.fc3   = QuantLinear(hidden2, hidden3, bias=True, weight_bit_width=8)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(hidden3)\n",
    "        self.relu3 = QuantReLU(bit_width=8)\n",
    "        \n",
    "        self.out   = QuantLinear(hidden3, num_classes, bias=False, weight_bit_width=8)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(num_classes)\n",
    "        #self.relu4 = QuantReLU(bit_width=1, max_val=1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.batchnorm1(out)\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = self.batchnorm2(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        out = self.fc3(out)\n",
    "        out = self.batchnorm3(out)\n",
    "        out = self.relu3(out)\n",
    "        \n",
    "        out = self.out(out)\n",
    "        out = self.batchnorm4(out)\n",
    "        return out\n",
    "    \n",
    "model = QuantLeNet(input_size, hidden1, hidden2, hidden3, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define loss and optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize UNSW_NB15 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([175341, 197])\n",
      "torch.Size([82332, 197])\n",
      "torch.Size([175341, 197])\n",
      "torch.Size([82332, 197])\n"
     ]
    }
   ],
   "source": [
    "#these are not slitted into validation and train set\n",
    "train_dataset = UNSW_NB15(file_path ='data/UNSW_NB15_training-set.csv')\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#get the test dataframe\n",
    "test_dataset = UNSW_NB15(file_path ='data/UNSW_NB15_testing-set.csv')\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#Get the Quantized versions\n",
    "train_quantized_dataset = UNSW_NB15_quantized(file_path_train='data/UNSW_NB15_training-set.csv', \\\n",
    "                                              file_path_test = \"data/UNSW_NB15_testing-set.csv\", \\\n",
    "                                              train=True)\n",
    "train_quantized_loader = DataLoader(train_quantized_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_quantized_dataset = UNSW_NB15_quantized(file_path_train='data/UNSW_NB15_training-set.csv', \\\n",
    "                                              file_path_test = \"data/UNSW_NB15_testing-set.csv\", \\\n",
    "                                              train=False)\n",
    "test_quantized_loader = DataLoader(test_quantized_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Train, Test the model and see the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_quantized_loader\n",
    "test_loader = test_quantized_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 1/2 [00:10<00:10, 10.94s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 2/2 [00:21<00:00, 10.73s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n",
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "running_loss = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "        loss_epoch = train(model, device, train_loader, optimizer,criterion)\n",
    "        running_loss.append(loss_epoch)\n",
    "#Save the model!!\n",
    "torch.save(model.state_dict(), \"MLP_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check if output has nans= tensor(1, dtype=torch.uint8)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-1c078fa1875f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#model.load_state_dict(torch.load(\"MLP_model\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-65-ce0f16440062>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, device, test_loader)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Check if output has nans=\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m#PREDICTIONS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "#model.load_state_dict(torch.load(\"MLP_model\"))\n",
    "test(model,device,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_per_epoch = [np.mean(loss_per_epoch) for loss_per_epoch in running_loss]\n",
    "display_loss_plot(loss_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**********************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    num_epochs = 200\n",
    "    batch_size = 8000 \n",
    "    lr = 0.001\n",
    "    accuracy on test set = 0.7235218384103387\n",
    "<img src=\"data/train_not_quantized_72.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
