{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Quantized MLP on UNSW-NB15 with Brevitas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will show how to create, train and export a quantized Multi Layer Perceptron (MLP) with quantized weights and activations with [Brevitas](https://github.com/Xilinx/brevitas).\n",
    "Specifically, the task at hand will be to label network packets as normal or suspicious (e.g. originating from an attacker, virus, malware or otherwise) by training on a quantized variant of the UNSW-NB15 dataset. \n",
    "\n",
    "**You won't need a GPU to train the neural net.** This MLP will be small enough to train on a modern x86 CPU, so no GPU is required to follow this tutorial \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick introduction to the task and the dataset\n",
    "\n",
    "*The task:* The goal of [*network intrusion detection*](https://ieeexplore.ieee.org/abstract/document/283931) is to identify, preferably in real time, unauthorized use, misuse, and abuse of computer systems by both system insiders and external penetrators. This may be achieved by a mix of techniques, and machine-learning (ML) based techniques are increasing in popularity. \n",
    "\n",
    "*The dataset:* Several datasets are available for use in ML-based methods for intrusion detection.\n",
    "The [UNSW-NB15](https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/) is one such dataset created by the Australian Centre for Cyber Security (ACCS) to provide a comprehensive network based data set which can reflect modern network traffic scenarios. You can find more details about the dataset on [its homepage](https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/).\n",
    "\n",
    "*Performance considerations:* FPGAs are commonly used for implementing high-performance packet processing systems that still provide a degree of programmability. To avoid introducing bottlenecks on the network, the DNN implementation must be capable of detecting malicious ones at line rate, which can be millions of packets per second, and is expected to increase further as next-generation networking solutions provide increased\n",
    "throughput. This is a good reason to consider FPGA acceleration for this particular use-case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "-------------\n",
    "\n",
    "TODO reorder\n",
    "\n",
    "* [Define Helper Functions and Parameters ](#auxilary_&_parameters)\n",
    "* [Define the quantized MLP model class](#create_model)\n",
    "* [Define the Train and Test methods](#train_test) \n",
    "* [Define the Loss and Optimizer](#loss_optimizer)\n",
    "* [Load the UNSW_NB15 dataset](#load_dataset)\n",
    "* [Train, Test and see the Loss](#train_test_loss)\n",
    "* [Change the model structure after training](#change_model)\n",
    "* [Brevitas export](#brevitas_export)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup\n",
    "\n",
    "Let's start by making sure we have all the Python packages we'll need for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /workspace/.local/lib/python3.6/site-packages (1.1.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /opt/conda/lib/python3.6/site-packages (from pandas) (1.19.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: scikit-learn in /workspace/.local/lib/python3.6/site-packages (0.23.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.6/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /workspace/.local/lib/python3.6/site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.6/site-packages (from scikit-learn) (1.19.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /workspace/.local/lib/python3.6/site-packages (from scikit-learn) (1.0.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (4.31.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user pandas\n",
    "!pip install --user scikit-learn\n",
    "!pip install --user tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Quantized MLP model <a id='1create_model'></a>\n",
    "\n",
    "We'll now define an MLP model that will be trained to perform inference with quantized weights and activations.\n",
    "For this, we'll use the quantization-aware training (QAT) capabilities offered by[Brevitas](https://github.com/Xilinx/brevitas).\n",
    "\n",
    "Our MLP will have four fully-connected (FC) layers in total: three hidden layers with 64 neurons, and a final output layer with a single output, all using 2-bit weights. We'll use 2-bit quantized ReLU activation functions, and apply batch normalization between each FC layer and its activation.\n",
    "\n",
    "In case you'd like to experiment with different quantization settings or topology parameters, we'll keep define all these topology settings as variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 593      \n",
    "hidden1 = 64      \n",
    "hidden2 = 64\n",
    "hidden3 = 64\n",
    "weight_bit_width = 1\n",
    "act_bit_width = 2\n",
    "num_classes = 1    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our quantization settings to be used by Brevitas. We'll use a [dependency injection](https://en.wikipedia.org/wiki/Dependency_injection)-based method for setting up the quantization properties, which is also used in the training scripts for the [Brevitas BNN-PYNQ examples](https://github.com/Xilinx/brevitas/tree/master/brevitas_examples/bnn_pynq). This will enable setting up good default settings depending on which bitwidth(s) we end up using for our MLP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dependencies import value\n",
    "\n",
    "from brevitas.inject import BaseInjector as Injector\n",
    "from brevitas.core.bit_width import BitWidthImplType\n",
    "from brevitas.core.quant import QuantType\n",
    "from brevitas.core.restrict_val import RestrictValueType\n",
    "from brevitas.core.scaling import ScalingImplType\n",
    "\n",
    "class CommonQuant(Injector):\n",
    "    bit_width_impl_type = BitWidthImplType.CONST\n",
    "    scaling_impl_type = ScalingImplType.CONST\n",
    "    restrict_scaling_type = RestrictValueType.FP\n",
    "    scaling_per_output_channel = False\n",
    "    narrow_range = True\n",
    "    signed = True\n",
    "\n",
    "    @value\n",
    "    def quant_type(bit_width):\n",
    "        if bit_width is None:\n",
    "            return QuantType.FP\n",
    "        elif bit_width == 1:\n",
    "            return QuantType.BINARY\n",
    "        else:\n",
    "            return QuantType.INT\n",
    "\n",
    "class CommonWeightQuant(CommonQuant):\n",
    "    scaling_const = 1.0\n",
    "\n",
    "class CommonActQuant(CommonQuant):\n",
    "    min_val = -1.0\n",
    "    max_val = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define our MLP using the layer primitives provided by Brevitas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.nn import QuantLinear, QuantIdentity\n",
    "import torch.nn as nn\n",
    "\n",
    "act_layer = QuantIdentity\n",
    "\n",
    "class QuantMLP(nn.Module):\n",
    "    def __init__(self, input_size,hidden1, hidden2, hidden3, num_classes):\n",
    "        super(QuantMLP, self).__init__()\n",
    "        self.fc1 = QuantLinear(input_size, hidden1, bias=False, weight_bit_width=weight_bit_width, weight_quant=CommonWeightQuant)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(hidden1)\n",
    "        self.act1 = act_layer(act_quant=CommonActQuant, bit_width=act_bit_width)\n",
    "        \n",
    "        self.fc2 = QuantLinear(hidden1, hidden2, bias=False, weight_bit_width=weight_bit_width, weight_quant=CommonWeightQuant)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(hidden2)\n",
    "        self.act2 = act_layer(act_quant=CommonActQuant, bit_width=act_bit_width)\n",
    "        \n",
    "        self.fc3 = QuantLinear(hidden2, hidden3, bias=False, weight_bit_width=weight_bit_width, weight_quant=CommonWeightQuant)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(hidden3)\n",
    "        self.act3 = act_layer(act_quant=CommonActQuant, bit_width=act_bit_width)\n",
    "        \n",
    "        self.fc4 = QuantLinear(hidden3, num_classes, bias=True, weight_bit_width=weight_bit_width, weight_quant=CommonWeightQuant)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        fc1 = self.fc1(x)\n",
    "        b1 = self.batchnorm1(fc1)\n",
    "        act1 = self.act1(b1)\n",
    "        \n",
    "        fc2 = self.fc2(act1)\n",
    "        b2 = self.batchnorm2(fc2)\n",
    "        act2 = self.act2(b2)\n",
    "\n",
    "        fc3 = self.fc3(act2)\n",
    "        b3 = self.batchnorm3(fc3)\n",
    "        act3 = self.act3(b3)\n",
    "        \n",
    "        fc4 = self.fc4(act3)\n",
    "        return fc4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the MLP's output is not yet quantized. Even though we want the final output of our MLP to be a binary (0/1) value indicating the classification, we've only defined a single-neuron FC layer as the output. While training the network we'll pass that output through a sigmoid function as part of the loss criterion, which [gives better numerical stability](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html). Later on, after we're done training the network, we'll add a quantization node at the end before we export it to FINN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the UNSW_NB15 dataset <a id='load_dataset'></a>\n",
    "\n",
    "### Dataset quantization <a id='dataset_qnt'></a>\n",
    "\n",
    "The goal of this notebook is to train a Quantized Neural Network (QNN) to be later deployed as an FPGA accelerator generated by the FINN compiler. Although we can choose a variety of different precisions for the input, [Murovic and Trost](https://ev.fe.uni-lj.si/1-2-2019/Murovic.pdf) have previously shown we can actually binarize the inputs and still get good accuracy.\n",
    "Thus, we will create a binarized representation for the dataset by following the procedure defined by [Murovic and Trost](https://ev.fe.uni-lj.si/1-2-2019/Murovic.pdf), which we repeat briefly here:\n",
    "\n",
    "* Original features have different formats ranging from integers, floating numbers to strings.\n",
    "* Integers, which for example represent a packet lifetime, are binarized with as many bits as to include the maximum value. \n",
    "* Another case is with features formatted as strings (protocols), which are binarized by simply counting the number of all different strings for each feature and coding them in the appropriate number of bits.\n",
    "* Floating-point numbers are reformatted into fixed-point representation.\n",
    "* In the end, each sample is transformed into a 593-bit wide binary vector. \n",
    "* All vectors are labeled as bad (0) or normal (1)\n",
    "\n",
    "Following their open-source implementation provided as a Matlab script [here](https://github.com/TadejMurovic/BNN_Deployment/blob/master/cybersecurity_dataset_unswb15.m), we've created a [Python version](dataloader_quantized.py).\n",
    "This `UNSW_NB15_quantized` class implements a PyTorch `DataLoader`, which represents a Python iterable over a dataset. This is useful because enables access to data in batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the training and test set from the [official website](https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-12-15 01:22:25--  https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/a%20part%20of%20training%20and%20testing%20set/UNSW_NB15_training-set.csv\n",
      "Resolving www.unsw.adfa.edu.au (www.unsw.adfa.edu.au)... 202.58.60.197\n",
      "Connecting to www.unsw.adfa.edu.au (www.unsw.adfa.edu.au)|202.58.60.197|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 32293018 (31M) [text/csv]\n",
      "Saving to: 'UNSW_NB15_training-set.csv.1'\n",
      "\n",
      "UNSW_NB15_training- 100%[===================>]  30.80M   245KB/s    in 2m 39s  \n",
      "\n",
      "2020-12-15 01:25:06 (198 KB/s) - 'UNSW_NB15_training-set.csv.1' saved [32293018/32293018]\n",
      "\n",
      "--2020-12-15 01:25:06--  https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/a%20part%20of%20training%20and%20testing%20set/UNSW_NB15_testing-set.csv\n",
      "Resolving www.unsw.adfa.edu.au (www.unsw.adfa.edu.au)... 202.58.60.197\n",
      "Connecting to www.unsw.adfa.edu.au (www.unsw.adfa.edu.au)|202.58.60.197|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 15380800 (15M) [text/csv]\n",
      "Saving to: 'UNSW_NB15_testing-set.csv.1'\n",
      "\n",
      "UNSW_NB15_testing-s 100%[===================>]  14.67M   181KB/s    in 96s     \n",
      "\n",
      "2020-12-15 01:26:45 (156 KB/s) - 'UNSW_NB15_testing-set.csv.1' saved [15380800/15380800]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/a%20part%20of%20training%20and%20testing%20set/UNSW_NB15_training-set.csv\n",
    "! wget https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/a%20part%20of%20training%20and%20testing%20set/UNSW_NB15_testing-set.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from dataloader_quantized import UNSW_NB15_quantized\n",
    "\n",
    "file_path_train = \"UNSW_NB15_training-set.csv\"\n",
    "file_path_test = \"UNSW_NB15_testing-set.csv\"\n",
    "\n",
    "train_quantized_dataset = UNSW_NB15_quantized(file_path_train = file_path_train, \\\n",
    "                                              file_path_test = file_path_test, \\\n",
    "                                              train=True)\n",
    "\n",
    "test_quantized_dataset = UNSW_NB15_quantized(file_path_train = file_path_train, \\\n",
    "                                              file_path_test = file_path_test, \\\n",
    "                                              train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Train and Test  methods  <a id='train_test'></a>\n",
    "The train and test methods will use a `DataLoader`, which feeds the model with a new predefined batch of training data in each iteration, until the entire training data is fed to the model. Each repetition of this process is called an `epoch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion):\n",
    "    losses = []\n",
    "    # ensure model is in training mode\n",
    "    model.train()    \n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):        \n",
    "        inputs, target = data\n",
    "        optimizer.zero_grad()   \n",
    "                \n",
    "        # forward pass\n",
    "        output = model(inputs.float())\n",
    "        loss = criterion(output, target.unsqueeze(1))\n",
    "        \n",
    "        # backward pass + run optimizer to update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # keep track of loss value\n",
    "        losses.append(loss.data.numpy()) \n",
    "           \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):    \n",
    "    # ensure model is in eval mode\n",
    "    model.eval() \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, target = data\n",
    "            output_orig = model(inputs.float())\n",
    "            # run the output through sigmoid\n",
    "            output = torch.sigmoid(output_orig)  \n",
    "            # compare against a threshold of 0.5 to generate 0/1\n",
    "            pred = (output.detach().numpy() > 0.5) * 1\n",
    "            target = target.float()\n",
    "            y_true.extend(target.tolist()) \n",
    "            y_pred.extend(pred.reshape(-1).tolist())\n",
    "        \n",
    "    return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helper Functions and Parameters <a id='auxilary_&_parameters'></a>\n",
    "\n",
    "Before we start training our MLP we need to define some hyperparameters. Specifically the number of epochs, batch size and learning rate.\n",
    "Moreover, in order to monitor the loss function evolution over epochs, we need to define a method for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40\n",
    "batch_size = 100\n",
    "lr = 0.01 \n",
    "\n",
    "def display_loss_plot(losses, title=\"Training loss\", xlabel=\"Iterations\", ylabel=\"Loss\"):\n",
    "    x_axis = [i for i in range(len(losses))]\n",
    "    plt.plot(x_axis,losses)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Loss and Optimizer <a id=\"loss_optimizer\"></a>\n",
    "\n",
    "As mentioned earlier, we'll use a loss criterion which applies a sigmoid function during the training phase (`BCEWithLogitsLoss`). For the testing phase, we're manually computing the sigmoid and thresholding at 0.5 as seen above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx \n",
    "import torch\n",
    "\n",
    "# dataset loaders\n",
    "train_quantized_loader = DataLoader(train_quantized_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_quantized_loader = DataLoader(test_quantized_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# model to train\n",
    "model = QuantMLP(input_size, hidden1, hidden2, hidden3, num_classes)\n",
    "\n",
    "# loss criterion and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model and Verify the Loss <a id=\"train_test_loss\"></a>\n",
    "\n",
    "Now that we have everything defined, we can finally train the quantized MLP on the quantized dataset. Then, test its accuracy and watch the loss function evolution over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training loss:   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.206721 test accuracy = 0.820483:   2%|▎         | 1/40 [00:11<07:16, 11.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.160921 test accuracy = 0.828171:   5%|▌         | 2/40 [00:22<07:05, 11.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.153925 test accuracy = 0.837244:   8%|▊         | 3/40 [00:33<06:55, 11.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.152099 test accuracy = 0.829690:  10%|█         | 4/40 [00:45<06:45, 11.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.154176 test accuracy = 0.844070:  12%|█▎        | 5/40 [00:56<06:35, 11.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.150484 test accuracy = 0.838678:  15%|█▌        | 6/40 [01:07<06:24, 11.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.150323 test accuracy = 0.818697:  18%|█▊        | 7/40 [01:18<06:11, 11.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.146891 test accuracy = 0.837937:  20%|██        | 8/40 [01:30<06:00, 11.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.144228 test accuracy = 0.813414:  22%|██▎       | 9/40 [01:41<05:50, 11.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.141844 test accuracy = 0.860285:  25%|██▌       | 10/40 [01:52<05:38, 11.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.139459 test accuracy = 0.831839:  28%|██▊       | 11/40 [02:03<05:26, 11.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.142604 test accuracy = 0.810839:  30%|███       | 12/40 [02:15<05:15, 11.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.140008 test accuracy = 0.826617:  32%|███▎      | 13/40 [02:26<05:06, 11.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.142259 test accuracy = 0.808410:  35%|███▌      | 14/40 [02:38<04:54, 11.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.140141 test accuracy = 0.811701:  38%|███▊      | 15/40 [02:49<04:44, 11.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.143105 test accuracy = 0.828973:  40%|████      | 16/40 [03:00<04:32, 11.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.139130 test accuracy = 0.863747:  42%|████▎     | 17/40 [03:12<04:22, 11.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.133940 test accuracy = 0.868156:  45%|████▌     | 18/40 [03:23<04:10, 11.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.135788 test accuracy = 0.830661:  48%|████▊     | 19/40 [03:35<03:59, 11.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.138048 test accuracy = 0.886533:  50%|█████     | 20/40 [03:46<03:46, 11.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.136244 test accuracy = 0.822196:  52%|█████▎    | 21/40 [03:57<03:35, 11.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.136308 test accuracy = 0.823070:  55%|█████▌    | 22/40 [04:09<03:24, 11.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.132924 test accuracy = 0.885609:  57%|█████▊    | 23/40 [04:20<03:14, 11.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.131081 test accuracy = 0.888525:  60%|██████    | 24/40 [04:32<03:02, 11.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.133813 test accuracy = 0.879294:  62%|██████▎   | 25/40 [04:43<02:51, 11.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.133536 test accuracy = 0.876816:  65%|██████▌   | 26/40 [04:54<02:40, 11.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.131778 test accuracy = 0.832362:  68%|██████▊   | 27/40 [05:06<02:28, 11.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.138220 test accuracy = 0.868168:  70%|███████   | 28/40 [05:17<02:17, 11.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.135467 test accuracy = 0.864451:  72%|███████▎  | 29/40 [05:29<02:05, 11.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.136560 test accuracy = 0.868836:  75%|███████▌  | 30/40 [05:40<01:53, 11.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.135325 test accuracy = 0.850289:  78%|███████▊  | 31/40 [05:52<01:43, 11.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.131879 test accuracy = 0.873682:  80%|████████  | 32/40 [06:03<01:32, 11.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.130507 test accuracy = 0.818442:  82%|████████▎ | 33/40 [06:15<01:20, 11.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.132760 test accuracy = 0.831852:  85%|████████▌ | 34/40 [06:26<01:08, 11.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.131189 test accuracy = 0.821698:  88%|████████▊ | 35/40 [06:38<00:57, 11.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.132866 test accuracy = 0.862119:  90%|█████████ | 36/40 [06:49<00:45, 11.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.130355 test accuracy = 0.872516:  92%|█████████▎| 37/40 [07:01<00:34, 11.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.131844 test accuracy = 0.876111:  95%|█████████▌| 38/40 [07:12<00:22, 11.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.131016 test accuracy = 0.872893:  98%|█████████▊| 39/40 [07:23<00:11, 11.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.130985 test accuracy = 0.816001: 100%|██████████| 40/40 [07:35<00:00, 11.43s/it]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "running_loss = []\n",
    "running_test_acc = []\n",
    "t = trange(num_epochs, desc=\"Training loss\", leave=True)\n",
    "\n",
    "for epoch in t:\n",
    "        %prun loss_epoch = train(model, train_quantized_loader, optimizer,criterion)\n",
    "        test_acc = test(model, test_quantized_loader)\n",
    "        t.set_description(\"Training loss = %f test accuracy = %f\" % (np.mean(loss_epoch), test_acc))\n",
    "        t.refresh() # to show immediately the update           \n",
    "        running_loss.append(loss_epoch)\n",
    "        running_test_acc.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8160010688432201"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model, test_quantized_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss_per_epoch = [np.mean(loss_per_epoch) for loss_per_epoch in running_loss]\n",
    "display_loss_plot(loss_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change  the model after training <a id=\"change_model\"></a>\n",
    "\n",
    "Now that the model is trained, its output ranges {0,+1}. However, for implementation purposes we need for the output to belong to {-1,+1}. Therefore we need to add a layer before its output, such as a Quantized Identity layer. \n",
    "\n",
    "Furthermore, the input will be changed, so instead of the {0,+1} range, the model will now accept {-1,1} as valid input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the state_dict\n",
    "path = \"quantized_mlp_unsw_nb15.pt\"\n",
    "torch.save(model.state_dict(), path)\n",
    "\n",
    "# load the state_dict and create new model with aditional QuantIdentity layer\n",
    "new_model_to_be = QuantMLP(input_size, hidden1, hidden2, hidden3, num_classes)\n",
    "new_model_to_be.load_state_dict(torch.load(path))  \n",
    "new_model_to_be.eval()\n",
    "\n",
    "class extended_model(nn.Module):\n",
    "    def __init__(self, my_pretrained_model):\n",
    "        super(extended_model, self).__init__()\n",
    "        self.pretrained = my_pretrained_model\n",
    "        self.identity = QuantIdentity(act_quant=CommonActQuant, bit_width=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = (x + torch.tensor([1.0])) / 2.0  # shift from {-1,1} {0,1} \n",
    "        out_original = self.pretrained(x)\n",
    "        out_final = self.identity(out_original)   # output as {-1,1}     \n",
    "        return out_final\n",
    "\n",
    "new_model = extended_model(my_pretrained_model=new_model_to_be)\n",
    "new_model.eval()\n",
    "new_model_output = new_model.forward(test_quantized_dataset.data[:,:-1] * 2.0 - torch.tensor([1.0])) # feed data as {-1,1}\n",
    "new_model_output = new_model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brevitas export <a id=\"brevitas_export\" ></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINN expects an ONNX model as input. This can be a model trained with [Brevitas](https://github.com/Xilinx/brevitas). First a few things have to be imported. Then the model can be loaded with the pretrained weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to brevitas_w1_a2NSW_NB15_model.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:17: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n"
     ]
    }
   ],
   "source": [
    "import brevitas.onnx as bo\n",
    "\n",
    "export_onnx_path = \"brevitas_w%d_a%-uNSW_NB15_model.onnx\" % (weight_bit_width, act_bit_width)\n",
    "input_shape = (1, 593)\n",
    "bo.export_finn_onnx(new_model, input_shape, export_onnx_path)\n",
    "\n",
    "new_model.eval\n",
    "print(\"Model saved to %s\" % export_onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's it!\n",
    "You created, trained and tested a quantized MLP that is ready to be loaded into FINN! Congratulations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
